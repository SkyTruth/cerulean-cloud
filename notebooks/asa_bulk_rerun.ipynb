{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from geoalchemy2 import WKTElement\n",
    "import requests\n",
    "import movingpandas as mpd\n",
    "import shapely\n",
    "from shapely import frechet_distance\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\ebeva\\SkyTruth\\git\\cerulean-cloud\")\n",
    "load_dotenv(r\"C:\\Users\\ebeva\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerulean_cloud.cloud_function_asa.utils.analyzer import AISAnalyzer\n",
    "\n",
    "from cerulean_cloud.cloud_function_asa.utils.scoring import (\n",
    "    compute_distance_score,\n",
    "    compute_overlap_score,\n",
    "    compute_temporal_score,\n",
    "    compute_total_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_scene(scene_id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a S1 scene GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.sentinel1_grd/items?scene_id={scene_id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{scene_id}.geojson\")\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for Scene {scene_id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "    s1_gdf = gpd.read_file(geojson_file_path)\n",
    "    s1_scene = SimpleNamespace(\n",
    "        scene_id=scene_id,\n",
    "        scihub_ingestion_time=s1_gdf.scihub_ingestion_time.iloc[0],\n",
    "        start_time=s1_gdf.start_time.iloc[0],\n",
    "        end_time=s1_gdf.end_time.iloc[0],\n",
    "        geometry=WKTElement(str(s1_gdf.geometry.iloc[0])),\n",
    "    )\n",
    "    return s1_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_geojson(id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "\n",
    "    Parameters:\n",
    "    - id (int): The unique identifier for the GeoJSON item.\n",
    "    - download_path (str): The directory path where the GeoJSON will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - geojson_file_path (str): The file path to the downloaded GeoJSON.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.slick/items?id={id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{id}.geojson\")\n",
    "\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for ID {id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "\n",
    "    return geojson_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_scene_id_from_inactive_slick(slick_id):\n",
    "    geojson_file_path = download_geojson(slick_id)\n",
    "    slick_gdf = gpd.read_file(geojson_file_path)\n",
    "    orch_id = slick_gdf[\"orchestrator_run\"].values[0]\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.orchestrator_run/items?id={orch_id}\"\n",
    "    orch_data = gpd.GeoDataFrame.from_features(requests.get(url).json()[\"features\"])\n",
    "    s1_id = orch_data[\"sentinel1_grd\"].values[0]\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.sentinel1_grd/items?id={s1_id}\"\n",
    "    s1_data = gpd.GeoDataFrame.from_features(requests.get(url).json())\n",
    "    return s1_data[\"scene_id\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vess_infra_true_ranks(df, coll_column=\"collated_score\"):\n",
    "    vessel_true_rank = []\n",
    "    infra_true_rank = []\n",
    "    for slick_id, group in df.groupby(\"slick\"):\n",
    "        group.sort_values(by=coll_column, ascending=False, inplace=True)\n",
    "        group[\"calculated_rank\"] = list(range(1, len(group) + 1))\n",
    "        true_source = group[group[\"hitl_verification\"]]\n",
    "        if len(true_source) != 1:\n",
    "            continue\n",
    "            # raise BaseException(\"can't have more than one true source per slick\")\n",
    "\n",
    "        calc_rank = true_source[\"calculated_rank\"].values[0]\n",
    "        if true_source[\"type\"].values[0] == 1:\n",
    "            vessel_true_rank.append(calc_rank)\n",
    "        else:\n",
    "            infra_true_rank.append(calc_rank)\n",
    "    return vessel_true_rank, infra_true_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_distance_score(\n",
    "    traj: mpd.Trajectory,\n",
    "    curves: gpd.GeoDataFrame,\n",
    "    crs_meters: str,\n",
    "    ais_ref_dist: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the frechet distance between an AIS trajectory and an oil slick curve\n",
    "\n",
    "    Args:\n",
    "        traj (mpd.Trajectory): AIS trajectory\n",
    "        curves (gpd.GeoDataFrame): oil slick curves\n",
    "\n",
    "    Returns:\n",
    "        float: frechet distance between traj and curve\n",
    "    \"\"\"\n",
    "    # Only use the longest curve\n",
    "\n",
    "    frechet_scores = []\n",
    "    # curve_lengths = []\n",
    "\n",
    "    # print(\"Processing \", len(curves), \"curves...\")\n",
    "    for i, slick_curve in curves.to_crs(crs_meters).iterrows():\n",
    "        curve = slick_curve[\"geometry\"]  # curves.to_crs(crs_meters).iloc[0][\"geometry\"]\n",
    "        curve_length = slick_curve[\"length\"]\n",
    "        # print(\"Working with \", type(curve))\n",
    "        # get the trajectory coordinates as points in descending time order from collect\n",
    "        traj_gdf = (\n",
    "            traj.to_point_gdf()\n",
    "            .sort_values(by=\"timestamp\", ascending=False)\n",
    "            .set_crs(\"4326\")\n",
    "            .to_crs(crs_meters)\n",
    "        )\n",
    "\n",
    "        # take the points and put them in a linestring\n",
    "        traj_line = shapely.geometry.LineString(traj_gdf.geometry)\n",
    "\n",
    "        # get the first and last points of the slick curve\n",
    "        first_point = shapely.geometry.Point(curve.coords[0])\n",
    "        last_point = shapely.geometry.Point(curve.coords[-1])\n",
    "\n",
    "        # compute the distance from these points to the start of the trajectory\n",
    "        first_dist = first_point.distance(shapely.geometry.Point(traj_line.coords[0]))\n",
    "        last_dist = last_point.distance(shapely.geometry.Point(traj_line.coords[0]))\n",
    "\n",
    "        if last_dist < first_dist:\n",
    "            # change input orientation by reversing the slick curve\n",
    "            curve = shapely.geometry.LineString(list(curve.coords)[::-1])\n",
    "\n",
    "        # for every point in the curve, find the closest trajectory point and store it off\n",
    "        traj_points = list()\n",
    "        for curve_point in curve.coords:\n",
    "            # compute the distance between this point and every point in the trajectory\n",
    "            these_distances = list()\n",
    "            for traj_point in traj_line.coords:\n",
    "                dist = shapely.geometry.Point(curve_point).distance(\n",
    "                    shapely.geometry.Point(traj_point)\n",
    "                )\n",
    "                these_distances.append(dist)\n",
    "\n",
    "            closest_distance = min(these_distances)\n",
    "            closest_idx = these_distances.index(closest_distance)\n",
    "            traj_points.append(shapely.geometry.Point(traj_line.coords[closest_idx]))\n",
    "\n",
    "        # compute the frechet distance between the sampled trajectory curve and the slick curve\n",
    "        traj_line_clip = shapely.geometry.LineString(traj_points)\n",
    "        dist = frechet_distance(traj_line_clip, curve)\n",
    "\n",
    "        frechet_score = curve_length * math.exp(-dist / ais_ref_dist)\n",
    "        frechet_scores.append(frechet_score)\n",
    "\n",
    "    # print(frechet_scores)\n",
    "    # print(curves['length'])\n",
    "    return np.sum(frechet_scores) / np.sum(curves[\"length\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSpineASA(AISAnalyzer):\n",
    "    def __init__(self, s1_scene, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the custom AISAnalyzer.\n",
    "        \"\"\"\n",
    "        super().__init__(s1_scene, **kwargs)\n",
    "\n",
    "    def compute_coincidence_scores(\n",
    "        self,\n",
    "        slick_gdf: gpd.GeoDataFrame,\n",
    "        compute_custom_distance_score=None,\n",
    "        focus_st_list=[],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Associates AIS trajectories with slicks.\n",
    "        \"\"\"\n",
    "        self.results = gpd.GeoDataFrame()\n",
    "\n",
    "        self.slick_curves = None\n",
    "        self.slick_gdf = slick_gdf\n",
    "\n",
    "        if self.ais_gdf is None:\n",
    "            self.retrieve_ais_data()\n",
    "        if self.ais_gdf.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        self.slick_to_curves()\n",
    "        if self.ais_trajectories is None:\n",
    "            self.build_trajectories()\n",
    "        if self.ais_buffered is None:\n",
    "            self.buffer_trajectories()\n",
    "        if compute_custom_distance_score is None:\n",
    "            self.score_trajectories()\n",
    "        else:\n",
    "            self.score_trajectories(compute_custom_distance_score)\n",
    "        self.results[\"collated_score\"] = (\n",
    "            self.results[\"coincidence_score\"] - self.coinc_mean\n",
    "        ) / self.coinc_std\n",
    "        return self.results\n",
    "\n",
    "    def score_trajectories(self, compute_custom_distance_score=None, focus_st=None):\n",
    "        \"\"\"\n",
    "        Measure association by computing multiple metrics between AIS trajectories and slicks\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame of slick associations\n",
    "        \"\"\"\n",
    "        # print(\"Scoring trajectories\")\n",
    "\n",
    "        columns = [\n",
    "            \"st_name\",\n",
    "            \"ext_id\",\n",
    "            \"geometry\",\n",
    "            \"coincidence_score\",\n",
    "            \"type\",\n",
    "            \"ext_name\",\n",
    "            \"ext_shiptype\",\n",
    "            \"flag\",\n",
    "            \"overlap_score\",\n",
    "            \"distance_score\",\n",
    "            \"temporal_score\",\n",
    "        ]\n",
    "\n",
    "        # Create a GeoDataFrame of buffered trajectories\n",
    "        buffered_trajectories_gdf = self.ais_buffered.copy()\n",
    "        buffered_trajectories_gdf[\"id\"] = [t.id for t in self.ais_trajectories]\n",
    "        buffered_trajectories_gdf.set_index(\"id\", inplace=True)\n",
    "\n",
    "        # Perform a spatial join between buffered trajectories and slick geometries\n",
    "        matches = gpd.sjoin(\n",
    "            buffered_trajectories_gdf,\n",
    "            self.slick_gdf,\n",
    "            how=\"inner\",\n",
    "            predicate=\"intersects\",\n",
    "        )\n",
    "\n",
    "        if matches.empty:\n",
    "            print(\"No trajectories intersect the slicks.\")\n",
    "            self.results = gpd.GeoDataFrame(columns=columns, crs=\"4326\")\n",
    "            return self.results\n",
    "\n",
    "        # Get unique trajectory IDs that intersect slicks\n",
    "        intersecting_traj_ids = matches.index.unique()\n",
    "\n",
    "        # Filter trajectories and weights based on intersecting IDs\n",
    "        ais_filt = [t for t in self.ais_trajectories if t.id in intersecting_traj_ids]\n",
    "        weighted_filt = [\n",
    "            self.ais_weighted[idx]\n",
    "            for idx, t in enumerate(self.ais_trajectories)\n",
    "            if t.id in intersecting_traj_ids\n",
    "        ]\n",
    "        buffered_filt = [buffered_trajectories_gdf.loc[[t.id]] for t in ais_filt]\n",
    "\n",
    "        entries = []\n",
    "        # Skip the loop if weighted_filt is empty\n",
    "        if weighted_filt:\n",
    "            # Create a trajectory collection from filtered trajectories\n",
    "            ais_filt = mpd.TrajectoryCollection(ais_filt)\n",
    "\n",
    "            # Iterate over filtered trajectories\n",
    "            for t, w, b in zip(ais_filt, weighted_filt, buffered_filt):\n",
    "                if t.id != focus_st and focus_st is not None:\n",
    "                    continue\n",
    "                print(\"COMPUTING SCORES FOR\", t.id)\n",
    "                # Compute temporal score\n",
    "                temporal_score = compute_temporal_score(w, self.slick_gdf)\n",
    "\n",
    "                # Compute overlap score\n",
    "                overlap_score = compute_overlap_score(\n",
    "                    b, self.slick_gdf, self.crs_meters\n",
    "                )\n",
    "\n",
    "                # Compute distance score between trajectory and slick curve\n",
    "                distance_score = compute_custom_distance_score(\n",
    "                    t, self.slick_curves, self.crs_meters, self.ais_ref_dist\n",
    "                )\n",
    "\n",
    "                # Compute total score from these three metrics\n",
    "                coincidence_score = compute_total_score(\n",
    "                    temporal_score,\n",
    "                    overlap_score,\n",
    "                    distance_score,\n",
    "                    self.w_temporal,\n",
    "                    self.w_overlap,\n",
    "                    self.w_distance,\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"st_name {t.id}: coincidence_score ({round(coincidence_score, 2)}) = \"\n",
    "                    f\"({self.w_overlap} * overlap_score ({round(overlap_score, 2)}) + \"\n",
    "                    f\"{self.w_temporal} * temporal_score ({round(temporal_score, 2)}) + \"\n",
    "                    f\"{self.w_distance} * distance_score ({round(distance_score, 2)})) / \"\n",
    "                    f\"({self.w_overlap + self.w_temporal + self.w_distance})\"\n",
    "                )\n",
    "\n",
    "                entry = {\n",
    "                    \"st_name\": t.id,\n",
    "                    \"ext_id\": str(t.id),\n",
    "                    \"geometry\": shapely.geometry.LineString(\n",
    "                        [p.coords[0] for p in t.df[\"geometry\"]]\n",
    "                    ),\n",
    "                    \"coincidence_score\": coincidence_score,\n",
    "                    \"type\": 1,  # Vessel\n",
    "                    \"ext_name\": t.ext_name,\n",
    "                    \"ext_shiptype\": t.ext_shiptype,\n",
    "                    \"flag\": t.flag,\n",
    "                    \"distance_score\": distance_score,\n",
    "                    \"overlap_score\": overlap_score,\n",
    "                    \"temporal_score\": temporal_score,\n",
    "                }\n",
    "                entries.append(entry)\n",
    "\n",
    "        sources = gpd.GeoDataFrame(entries, columns=columns, crs=\"4326\")\n",
    "        self.results = sources[sources[\"coincidence_score\"] >= 0]\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"C:\\Users\\ebeva\\SkyTruth\\cv3\\slick_to_source dump 2024-12-31.csv\"\n",
    "true_slick_df = pd.read_csv(csv_path)\n",
    "csv_path = r\"C:\\Users\\ebeva\\SkyTruth\\cv3\\asa_analysis\\slick_to_source False Positive dump 2025-01-3.csv\"\n",
    "false_slick_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(true_slick_df.groupby(\"scene_id\")))\n",
    "print(len(false_slick_df.groupby(\"scene_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [\n",
    "#     'scene_id', 'slick_id', 'source_id', 'is_fp_slick', 'hitl_verification',\n",
    "#     'type', 'coincidence_score', 'weighted_coin_score', 'overlap_score',\n",
    "#     'temporal_score', 'weighted_dist_score', 'distance_score'\n",
    "# ]\n",
    "\n",
    "# # Create an empty DataFrame with these columns\n",
    "# rerun_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(rerun_df)\n",
    "\n",
    "# for (scene_id, true_group) in tqdm(true_slick_df.groupby('scene_id')):\n",
    "#     if not np.any(true_group['hitl_verification'].values):\n",
    "#         continue\n",
    "\n",
    "#     print(scene_id)\n",
    "#     s1_scene = get_s1_scene(scene_id)\n",
    "#     asa = MultiSpineASA(s1_scene, hours_before = 8)\n",
    "\n",
    "#     false_group = false_slick_df[false_slick_df['scene_id']==scene_id]\n",
    "\n",
    "#     print(\"processing \",len(np.unique(true_group['slick'].values)), \"true slicks...\")\n",
    "#     print(\"processing \",len(np.unique(false_group['slick'].values)), \"false slicks...\")\n",
    "\n",
    "\n",
    "#     for slick_id,true_slick in true_group.groupby('slick'):\n",
    "#         geojson_file_path = download_geojson(slick_id)\n",
    "#         slick_gdf = gpd.read_file(geojson_file_path)\n",
    "#         res = asa.compute_coincidence_scores(slick_gdf,compute_distance_score)\n",
    "#         res_weighted = asa.compute_coincidence_scores(slick_gdf,compute_weighted_distance_score)\n",
    "\n",
    "#         for i,row in true_slick.iterrows():\n",
    "\n",
    "#             if row['type']==1:\n",
    "#                 # print(row.values)\n",
    "#                 sel_res = res[res['st_name']==str(row['st_name'])].iloc[0]\n",
    "#                 sel_res_weighted = res_weighted[res_weighted['st_name']==str(row['st_name'])].iloc[0]\n",
    "#                 coincidence_score, overlap_score, temporal_score, distance_score = sel_res['coincidence_score'], sel_res['overlap_score'], sel_res['temporal_score'], sel_res['distance_score']\n",
    "#                 weighted_dist_score, weighted_coin_score = sel_res_weighted['distance_score'], sel_res_weighted['coincidence_score']\n",
    "#             # else:\n",
    "#             #     coincidence_score, overlap_score, temporal_score, distance_score = sel_res['coincidence_score'],0,0,0\n",
    "#             #     weighted_dist_score, weighted_coin_score = 0, sel_res_weighted['coincidence_score']\n",
    "\n",
    "#                 insert_row = {\n",
    "#                     'scene_id':scene_id, 'slick_id':slick_id, 'source_id':row['source'], 'is_fp_slick':False, 'hitl_verification':row['hitl_verification'],\n",
    "#                     'type':row['type'], 'coincidence_score':coincidence_score, 'weighted_coin_score':weighted_coin_score, 'overlap_score':overlap_score,\n",
    "#                     'temporal_score':temporal_score, 'weighted_dist_score':weighted_dist_score, 'distance_score':distance_score\n",
    "#                 }\n",
    "#             # rerun_df.append(insert_row)\n",
    "#             new_row = pd.DataFrame([insert_row])\n",
    "#             rerun_df = pd.concat([rerun_df, new_row], ignore_index=True)\n",
    "#         pass\n",
    "#         # down_true = download_geojson(slick_id)\n",
    "#         # slick_gdf = gpd.read_file(down_true)\n",
    "#         # res = asa.compute_coincidence_scores(compute_distance_score)\n",
    "#         # res_weighted = asa.compute_coincidence_scores(compute_weighted_distance_score)\n",
    "\n",
    "#     # for slick_id,false_slick in false_group.groupby('slick'):\n",
    "#     #     pass\n",
    "#     #     down_false = download_geojson(slick_id)\n",
    "#     #     slick_gdf = gpd.read_file(down_false)\n",
    "#     #     res = asa.compute_coincidence_scores(slick_gdf,compute_distance_score)\n",
    "#     #     res_weighted = asa.compute_coincidence_scores(slick_gdf,compute_weighted_distance_score)\n",
    "#     #     for i,row in false_slick.iterrows():\n",
    "#     #         if row['type']==1:\n",
    "#     #             # print(row.values)\n",
    "#     #             sel_res = res[res['st_name']==str(row['st_name'])].iloc[0]\n",
    "#     #             sel_res_weighted = res_weighted[res_weighted['st_name']==str(row['st_name'])].iloc[0]\n",
    "#     #             coincidence_score, overlap_score, temporal_score, distance_score = sel_res['coincidence_score'], sel_res['overlap_score'], sel_res['temporal_score'], sel_res['distance_score']\n",
    "#     #             weighted_dist_score, weighted_coin_score = sel_res_weighted['distance_score'], sel_res_weighted['coincidence_score']\n",
    "#     #         else:\n",
    "#     #             coincidence_score, overlap_score, temporal_score, distance_score = sel_res['coincidence_score'],0,0,0\n",
    "#     #             weighted_dist_score, weighted_coin_score = 0, sel_res_weighted['coincidence_score']\n",
    "\n",
    "#     #         insert_row = {\n",
    "#     #             'scene_id':scene_id, 'slick_id':slick_id, 'source_id':row['source'], 'is_fp_slick':True, 'hitl_verification':row['hitl_verification'],\n",
    "#     #             'type':row['type'], 'coincidence_score':coincidence_score, 'weighted_coin_score':weighted_coin_score, 'overlap_score':overlap_score,\n",
    "#     #             'temporal_score':temporal_score, 'weighted_dist_score':weighted_dist_score, 'distance_score':distance_score\n",
    "#     #         }\n",
    "#     #         # rerun_df.append(insert_row)\n",
    "#     #         new_row = pd.DataFrame([insert_row])\n",
    "#     #         rerun_df = pd.concat([rerun_df, new_row], ignore_index=True)\n",
    "\n",
    "#     clear_output()\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURS_BEFORE = 8\n",
    "HOURS_AFTER = 2\n",
    "TIMESTEPS_PER_HOUR = 6\n",
    "NUM_TIMESTEPS = HOURS_BEFORE * TIMESTEPS_PER_HOUR\n",
    "\n",
    "# BUFFERING PARAMETERS FOR AIS\n",
    "AIS_PROJECT_ID = \"world-fishing-827\"\n",
    "AIS_BUFFER = 20000  # buffer around GRD envelope to capture AIS\n",
    "SPREAD_RATE = 1000  # meters/hour perpendicular to vessel track\n",
    "BUF_START = 100\n",
    "BUF_END = BUF_START + SPREAD_RATE * HOURS_BEFORE\n",
    "BUF_VEC = np.linspace(BUF_START, BUF_END, NUM_TIMESTEPS)\n",
    "\n",
    "# WEIGHTING PARAMETERS FOR AIS\n",
    "WEIGHT_START = 2.0\n",
    "WEIGHT_END = 0.0\n",
    "WEIGHT_VEC = np.linspace(WEIGHT_START, WEIGHT_END, NUM_TIMESTEPS) / NUM_TIMESTEPS\n",
    "\n",
    "W_TEMPORAL = 1.0\n",
    "W_OVERLAP = 1.0\n",
    "W_DISTANCE = 2.0\n",
    "AIS_REF_DIST = 4000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"scene_id\",\n",
    "    \"slick_id\",\n",
    "    \"st_name\",\n",
    "    \"coincidence_score\",\n",
    "    \"overlap_score\",\n",
    "    \"temporal_score\",\n",
    "    \"distance_score\",\n",
    "    \"coll_score\",\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "rerun_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(rerun_df)\n",
    "\n",
    "for scene_id, true_group in tqdm(true_slick_df.groupby(\"scene_id\")):\n",
    "    if not np.any(true_group[\"hitl_verification\"].values):\n",
    "        continue\n",
    "    print(scene_id)\n",
    "    s1_scene = get_s1_scene(scene_id)\n",
    "    asa = MultiSpineASA(\n",
    "        s1_scene,\n",
    "        hours_before=HOURS_BEFORE,\n",
    "        num_timesteps=NUM_TIMESTEPS,\n",
    "        buf_end=BUF_END,\n",
    "        buf_vec=BUF_VEC,\n",
    "        weight_vec=WEIGHT_VEC,\n",
    "        w_temporal=W_TEMPORAL,\n",
    "        w_overlap=W_OVERLAP,\n",
    "        w_distance=W_DISTANCE,\n",
    "    )\n",
    "\n",
    "    print(\"processing \", len(np.unique(true_group[\"slick\"].values)), \"true slicks...\")\n",
    "\n",
    "    for slick_id, true_slick in true_group.groupby(\"slick\"):\n",
    "        geojson_file_path = download_geojson(slick_id)\n",
    "        slick_gdf = gpd.read_file(geojson_file_path)\n",
    "        res = asa.compute_coincidence_scores(slick_gdf, compute_distance_score)\n",
    "\n",
    "        for i, sel_res in res.iterrows():\n",
    "            coincidence_score, overlap_score, temporal_score, distance_score = (\n",
    "                sel_res[\"coincidence_score\"],\n",
    "                sel_res[\"overlap_score\"],\n",
    "                sel_res[\"temporal_score\"],\n",
    "                sel_res[\"distance_score\"],\n",
    "            )\n",
    "            insert_row = {\n",
    "                \"scene_id\": scene_id,\n",
    "                \"slick_id\": slick_id,\n",
    "                \"st_name\": sel_res[\"st_name\"],\n",
    "                \"coincidence_score\": coincidence_score,\n",
    "                \"overlap_score\": overlap_score,\n",
    "                \"temporal_score\": temporal_score,\n",
    "                \"distance_score\": distance_score,\n",
    "                \"coll_score\": sel_res[\"collated_score\"],\n",
    "            }\n",
    "            new_row = pd.DataFrame([insert_row])\n",
    "            rerun_df = pd.concat([rerun_df, new_row], ignore_index=True)\n",
    "\n",
    "    clear_output()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
