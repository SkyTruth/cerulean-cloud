{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = \"TEST\"\n",
    "# stack = \"STAGING\"\n",
    "# stack = \"PRODUCTION\"\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import datetime\n",
    "import subprocess\n",
    "import math\n",
    "import httpx\n",
    "import json\n",
    "import boto3\n",
    "import tqdm\n",
    "import os\n",
    "import operator\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from google.cloud import tasks_v2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "\n",
    "(\n",
    "    NAME,\n",
    "    TITILER_URL,\n",
    "    TITILER_API_KEY,\n",
    "    API_KEY,\n",
    "    INFERENCE_URL,\n",
    "    DB_URL,\n",
    "    ASA_QUEUE,\n",
    "    ASA_URL,\n",
    "    SR_URL,\n",
    "    ORCHESTRATOR_URL,\n",
    ") = operator.itemgetter(\n",
    "    \"NAME\",\n",
    "    \"TITILER_URL\",\n",
    "    \"TITILER_API_KEY\",\n",
    "    \"API_KEY\",\n",
    "    \"INFERENCE_URL\",\n",
    "    \"DB_URL\",\n",
    "    \"ASA_QUEUE\",\n",
    "    \"ASA_URL\",\n",
    "    \"SR_URL\",\n",
    "    \"ORCHESTRATOR_URL\",\n",
    ")(\n",
    "    json.loads(os.getenv(\"STACK_SECRETS\"))[stack]\n",
    ")\n",
    "\n",
    "print((NAME + \"\\n\") * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM CSV\n",
    "scenes_csv = \"/Users/jonathanraphael/Downloads/planet_backlog_scene_ids.csv\"\n",
    "scenes_pd = pd.read_csv(scenes_csv)\n",
    "scenes = scenes_pd[\"s1_scene_id\"].tolist()\n",
    "print(len(scenes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM LIST\n",
    "scenes = [\n",
    "    \"S1A_IW_GRDH_1SDV_20241006T172800_20241006T172825_055985_06D8A6_318D\",\n",
    "]\n",
    "print(len(scenes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORCHESTRATOR --SLOW-- \n",
    "## (via Scene Relevancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FAKE_FOOTPRINT = False\n",
    "BATCH_SIZE = 100\n",
    "total_records = len(scenes)\n",
    "num_batches = math.ceil(total_records / BATCH_SIZE)\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def get_coordinates(scene_id, USE_FAKE_FOOTPRINT):\n",
    "    \"\"\"\n",
    "    Given a scene id, retrieve the corresponding productInfo.json and return the footprint coordinates.\n",
    "    \"\"\"\n",
    "    if USE_FAKE_FOOTPRINT:\n",
    "        return [\n",
    "            [\n",
    "                [0.0, 0.0],\n",
    "                [0.0, 1.0],\n",
    "                [1.0, 1.0],\n",
    "                [1.0, 0.0],\n",
    "                [0.0, 0.0],\n",
    "            ]\n",
    "        ]\n",
    "    file_key = f\"GRD/{scene_id[17:21]}/{int(scene_id[21:23])}/{int(scene_id[23:25])}/IW/DV/{scene_id}/productInfo.json\"\n",
    "    response = s3_client.get_object(\n",
    "        Bucket=\"sentinel-s1-l1c\",\n",
    "        Key=file_key,\n",
    "        RequestPayer=\"requester\",  # if required by your bucket settings\n",
    "    )\n",
    "    data = json.loads(response[\"Body\"].read())\n",
    "    return data[\"footprint\"][\"coordinates\"]\n",
    "\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"batch_num: {batch_num} of {num_batches}\")\n",
    "    batch = scenes[batch_num * BATCH_SIZE : (batch_num + 1) * BATCH_SIZE]\n",
    "    records = [\n",
    "        {\n",
    "            \"Sns\": {\n",
    "                \"Message\": json.dumps(\n",
    "                    {\n",
    "                        \"id\": scene_id,\n",
    "                        \"mode\": scene_id.split(\"_\")[1],\n",
    "                        \"resolution\": scene_id.split(\"_\")[2][-1],\n",
    "                        \"polarization\": scene_id.split(\"_\")[3][-2:],\n",
    "                        \"footprint\": {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": get_coordinates(\n",
    "                                scene_id, USE_FAKE_FOOTPRINT\n",
    "                            ),\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "        for scene_id in tqdm.tqdm(batch)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        orchestrator_result = httpx.post(\n",
    "            SR_URL,\n",
    "            json={\"Records\": records},\n",
    "            timeout=20.0,  # Adjusted timeout\n",
    "            headers={\"Authorization\": f\"Bearer {API_KEY}\"},\n",
    "        )\n",
    "        print(f\"Batch {batch_num + 1}/{num_batches}:\", orchestrator_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_num + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORCHESTRATOR --FAST--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene_id in tqdm.tqdm(scenes[:40]):\n",
    "    try:\n",
    "        orchestrator_result = httpx.post(\n",
    "            ORCHESTRATOR_URL + \"/orchestrate\",\n",
    "            json={\"scene_id\": f\"{scene_id}\"},\n",
    "            timeout=0.1,\n",
    "            headers={\"Authorization\": f\"Bearer {API_KEY}\"},\n",
    "        )\n",
    "        print(scene_id, orchestrator_result)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASA --SLOW--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def create_task(scene_id, parent):\n",
    "    # Define the payload for the task\n",
    "    payload = {\n",
    "        \"scene_id\": scene_id,\n",
    "        \"dry_run\": False,\n",
    "        \"overwrite_previous\": True,\n",
    "    }\n",
    "\n",
    "    # Define the task with an HTTP request\n",
    "    task = {\n",
    "        \"http_request\": {  # Specify the type of request\n",
    "            \"http_method\": tasks_v2.HttpMethod.POST,\n",
    "            \"url\": ASA_URL,  # The URL to send the request to\n",
    "            \"headers\": {\n",
    "                \"Content-type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "            },\n",
    "            \"body\": json.dumps(\n",
    "                payload\n",
    "            ).encode(),  # Convert payload to JSON and encode to bytes\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Create the task in the specified queue\n",
    "        response = client.create_task(request={\"parent\": parent, \"task\": task})\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating task for scene_id {scene_id}: {e}\")\n",
    "\n",
    "\n",
    "os.environ.pop(\"GOOGLE_APPLICATION_CREDENTIALS\", None)\n",
    "client = tasks_v2.CloudTasksClient()\n",
    "parent = client.queue_path(\"cerulean-338116\", \"europe-west1\", ASA_QUEUE)\n",
    "\n",
    "max_workers = 100  # Example of aggressive concurrency\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    results = list(\n",
    "        tqdm.tqdm(\n",
    "            executor.map(lambda scene_id: create_task(scene_id, parent), scenes),\n",
    "            total=len(scenes),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# for scene_id in tqdm.tqdm(scenes):\n",
    "#     create_task(scene_id, parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASA --FAST--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene_id in tqdm.tqdm(scenes):\n",
    "    try:\n",
    "        asa_result = httpx.post(\n",
    "            ASA_URL,\n",
    "            json={\n",
    "                \"scene_id\": f\"{scene_id}\",\n",
    "                \"dry_run\": False,\n",
    "                \"overwrite_previous\": True,\n",
    "                # \"run_flags\": [2], # 1: ais, 2: infra\n",
    "            },\n",
    "            timeout=0.1,\n",
    "            headers={\"Authorization\": f\"Bearer {API_KEY}\"},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Catalog/Examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS OpenDataRegistry ls and productInfo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download GeoTiffs from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GeoTiffs directly from S3 Open Data Registry using request payer and a list of Scene IDs\n",
    "\n",
    "import boto3\n",
    "import subprocess\n",
    "import tqdm\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"sentinel-s1-l1c\"\n",
    "\n",
    "for scene_id in tqdm.tqdm(scenes):\n",
    "    measurement = f\"GRD/{scene_id[17:21]}/{int(scene_id[21:23])}/{int(scene_id[23:25])}/IW/DV/{scene_id}/measurement/\"\n",
    "    try:\n",
    "        obj = s3_client.get_object(\n",
    "            Bucket=bucket_name, Key=measurement + \"iw-vv.tiff\", RequestPayer=\"requester\"\n",
    "        )\n",
    "        file_path = f\"/Users/jonathanraphael/git/ceruleanserver/local/temp/outputs/_rasters/{scene_id}.tiff\"\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            chunks = []\n",
    "            while data := obj[\"Body\"].read(1024):\n",
    "                chunks.append(data)\n",
    "            file.write(b\"\".join(chunks))\n",
    "\n",
    "        print(f\"{scene_id} downloaded successfully!\")\n",
    "    except:\n",
    "        command = (\n",
    "            f\"aws s3 ls s3://{bucket_name}/{measurement} --request-payer requester\"\n",
    "        )\n",
    "        output = subprocess.check_output(command, shell=True)\n",
    "        print(output)\n",
    "        print(f\"{scene_id} is missing the VV file. Perhaps one of the above instead?\")\n",
    "\n",
    "\n",
    "print(\"wrapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import boto3\n",
    "import json\n",
    "import subprocess\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "start_date = datetime.date(2023, 1, 10)\n",
    "end_date = datetime.date(2023, 1, 11)\n",
    "concurrency = 25\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"sentinel-s1-l1c\"\n",
    "current_date = start_date\n",
    "semaphore = asyncio.Semaphore(concurrency)\n",
    "\n",
    "\n",
    "def extract_folders(output):\n",
    "    lines = output.decode(\"utf-8\").split(\"\\n\")\n",
    "    folder_names = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"PRE\"):\n",
    "            folder_name = line.split()[1].strip(\"/\")\n",
    "            folder_names.append(folder_name)\n",
    "    return folder_names\n",
    "\n",
    "\n",
    "def append_to_file(folders, filename=\"output.txt\"):\n",
    "    with open(filename, \"a\") as file:\n",
    "        for folder in folders:\n",
    "            file.write(folder + \"\\n\")\n",
    "\n",
    "\n",
    "async def send_request(semaphore, session, stack, scene_id, coordinates, API_KEY):\n",
    "    async with semaphore:\n",
    "        url = f\"https://europe-west1-cerulean-338116.cloudfunctions.net/cerulean-cloud-{stack}-cf-sr\"\n",
    "        payload = {\n",
    "            \"Records\": [\n",
    "                {\n",
    "                    \"Sns\": {\n",
    "                        \"Message\": json.dumps(\n",
    "                            {\n",
    "                                \"id\": scene_id,\n",
    "                                \"mode\": scene_id.split(\"_\")[1],\n",
    "                                \"resolution\": scene_id.split(\"_\")[2][-1],\n",
    "                                \"polarization\": scene_id.split(\"_\")[3][-2:],\n",
    "                                \"footprint\": {\n",
    "                                    \"type\": \"Polygon\",\n",
    "                                    \"coordinates\": coordinates,\n",
    "                                },\n",
    "                            }\n",
    "                        )\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        try:\n",
    "            await session.post(url, json=payload, headers=headers, timeout=60.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for scene_id: {scene_id}\")\n",
    "            print(e)\n",
    "\n",
    "\n",
    "print(f\"RUNNING ON: {stack}\")\n",
    "while current_date <= end_date:\n",
    "    prefix = f\"GRD/{current_date.year}/{current_date.month}/{current_date.day}/IW/DV/\"\n",
    "    command = f\"aws s3 ls s3://{bucket_name}/{prefix} --request-payer requester\"\n",
    "    output = subprocess.check_output(command, shell=True)\n",
    "    folders = extract_folders(output)\n",
    "    id_footprint_dict = {}\n",
    "    for folder in tqdm.tqdm(folders):\n",
    "        file_key = f\"{prefix}{folder}/productInfo.json\"  # Ensure the correct path\n",
    "        try:\n",
    "            obj = s3_client.get_object(\n",
    "                Bucket=bucket_name, Key=file_key, RequestPayer=\"requester\"\n",
    "            )\n",
    "            content = obj[\"Body\"].read()\n",
    "            data = json.loads(content)\n",
    "            # Extract 'id' and 'footprint' and add to dictionary\n",
    "            # • store to GCP manifest\n",
    "            id_footprint_dict[data[\"id\"]] = data[\"footprint\"][\"coordinates\"]\n",
    "        except s3_client.exceptions.NoSuchKey:\n",
    "            print(f\"File not found: {file_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing file {file_key}: {e}\")\n",
    "\n",
    "    async with httpx.AsyncClient() as session:\n",
    "        tasks = [\n",
    "            send_request(semaphore, session, stack, k, v, API_KEY)\n",
    "            for k, v in id_footprint_dict.items()\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "    print(\"shipped\")\n",
    "\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "print(\"wrapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterates over the dates from start_date to end_date, lists S3 folders and collects the scene ID found in it.\n",
    "\"\"\"\n",
    "\n",
    "start_date = datetime.date(2025, 1, 16)\n",
    "end_date = datetime.date(2025, 1, 31)\n",
    "\n",
    "\n",
    "def extract_folders(output):\n",
    "    \"\"\"Extract folder names from an 'aws s3 ls' command output.\"\"\"\n",
    "    lines = output.decode(\"utf-8\").split(\"\\n\")\n",
    "    folder_names = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"PRE\"):\n",
    "            folder_name = line.split()[1].strip(\"/\")\n",
    "            folder_names.append(folder_name)\n",
    "    return folder_names\n",
    "\n",
    "\n",
    "scene_ids = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    prefix = f\"GRD/{current_date.year}/{current_date.month}/{current_date.day}/IW/DV/\"\n",
    "    command = f\"aws s3 ls s3://sentinel-s1-l1c/{prefix} --request-payer requester\"\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    try:\n",
    "        output = subprocess.check_output(command, shell=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing S3 objects for prefix {prefix}: {e}\")\n",
    "        continue\n",
    "\n",
    "    folders = extract_folders(output)\n",
    "    scene_ids.extend(folders)\n",
    "\n",
    "pd.DataFrame({\"s1_scene_id\": scene_ids}).to_csv(\n",
    "    \"/Users/jonathanraphael/Downloads/scene_ids.csv\", index=False\n",
    ")\n",
    "print(f\"len(scene_ids): {len(scene_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pulumi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
