{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import GeometryCollection, MultiPolygon, Polygon, mapping\n",
    "import os\n",
    "import pandas_gbq\n",
    "import shapely\n",
    "from google.oauth2.service_account import Credentials\n",
    "import json\n",
    "from shapely import wkt\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from geoalchemy2 import WKTElement\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from scipy.spatial import cKDTree\n",
    "import scipy.interpolate\n",
    "import scipy.spatial.distance\n",
    "import centerline.geometry\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\ebeva\\SkyTruth\\git\\cerulean-cloud\")\n",
    "load_dotenv(r\"C:\\Users\\ebeva\\.env\")\n",
    "\n",
    "from cerulean_cloud.cloud_function_ais_analysis.utils.analyzer import (  # noqa: E402\n",
    "    ASA_MAPPING,\n",
    "    InfrastructureAnalyzer,\n",
    "    AISAnalyzer,\n",
    "    SourceAnalyzer,\n",
    "    DarkAnalyzer,\n",
    ")\n",
    "\n",
    "from cerulean_cloud.cloud_function_ais_analysis.utils.constants import (\n",
    "    AIS_BUFFER,\n",
    "    AIS_PROJECT_ID,\n",
    "    AIS_REF_DIST,\n",
    "    BUF_VEC,\n",
    "    CLOSING_BUFFER,\n",
    "    D_FORMAT,\n",
    "    DECAY_FACTOR,\n",
    "    HOURS_AFTER,\n",
    "    HOURS_BEFORE,\n",
    "    INFRA_MEAN,\n",
    "    INFRA_REF_DIST,\n",
    "    INFRA_STD,\n",
    "    MIN_AREA_THRESHOLD,\n",
    "    NUM_TIMESTEPS,\n",
    "    NUM_VERTICES,\n",
    "    T_FORMAT,\n",
    "    VESSEL_MEAN,\n",
    "    VESSEL_STD,\n",
    "    W_DISTANCE,\n",
    "    W_OVERLAP,\n",
    "    W_TEMPORAL,\n",
    "    WEIGHT_VEC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_infrastructure_points(\n",
    "    slick_gdf, num_points, expansion_factor=0.2, crs=\"epsg:4326\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates random infrastructure points within an expanded bounding box of the combined geometry.\n",
    "\n",
    "    Parameters:\n",
    "    - slick_gdf (GeoDataFrame): GeoDataFrame containing slick polygons.\n",
    "    - num_points (int): Number of infrastructure points to generate.\n",
    "    - expansion_factor (float): Fraction to expand the bounding box.\n",
    "\n",
    "    Returns:\n",
    "    - infra_gdf (GeoDataFrame): GeoDataFrame of infrastructure points.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = slick_gdf.total_bounds\n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "    infra_x = np.random.uniform(\n",
    "        minx - expansion_factor * width, maxx + expansion_factor * width, num_points\n",
    "    )\n",
    "    infra_y = np.random.uniform(\n",
    "        miny - expansion_factor * height, maxy + expansion_factor * height, num_points\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"structure_start_date\": [pd.Timestamp(0)] * num_points,\n",
    "            \"structure_end_date\": [pd.Timestamp.now()] * num_points,\n",
    "        }\n",
    "    )\n",
    "    infra_gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(infra_x, infra_y), crs=crs\n",
    "    )\n",
    "    return infra_gdf\n",
    "\n",
    "\n",
    "def plot_coincidence(\n",
    "    analyzer,\n",
    "    slick_id,\n",
    "    black=True,\n",
    "    true_point=None,\n",
    "    nearby_vess=None, \n",
    "    padding_ratio = 0.2,\n",
    "    title = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a sample of infrastructure points with their coincidence scores.\n",
    "\n",
    "    Parameters:\n",
    "    - analyzer (SourceAnalyzer): Analyzer object containing infrastructure points and coincidence scores.\n",
    "    - slick_id (int): Identifier for the plot title.\n",
    "    - black (bool): Whether to use black borders for the infrastructure points.\n",
    "    \"\"\"\n",
    "\n",
    "    sample_size = len(analyzer.infra_gdf)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Create an axes object\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # First plot the infrastructure points\n",
    "    scatter = ax.scatter(\n",
    "        analyzer.infra_gdf.geometry.x[:sample_size],\n",
    "        analyzer.infra_gdf.geometry.y[:sample_size],\n",
    "        c=analyzer.coincidence_scores[:sample_size],\n",
    "        cmap=\"Blues\",\n",
    "        s=10,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        # alpha=analyzer.coincidence_scores[:sample_size],\n",
    "        edgecolor=\"black\" if black else None,  # Adds black borders\n",
    "        # linewidth=0.5,  # Optional: adjust border thickness\n",
    "        label=\"Coincidence Score Distribution\",\n",
    "    )\n",
    "\n",
    "    # Then plot the slick_gdf polygons on top\n",
    "    analyzer.slick_gdf.plot(\n",
    "        edgecolor=\"red\", linewidth=1, color=\"none\", ax=ax, label=\"Slick Polygons\"\n",
    "    )\n",
    "\n",
    "    if not true_point is None:\n",
    "        true_point.plot(\n",
    "            ax=ax, \n",
    "            color=\"none\",  # Ensure no fill color\n",
    "            edgecolor=\"none\",  # No edge color\n",
    "        )\n",
    "\n",
    "        # Add a green X marker using matplotlib\n",
    "        ax.scatter(\n",
    "            true_point.geometry.x, \n",
    "            true_point.geometry.y, \n",
    "            edgecolor='green',\n",
    "            color=\"none\", \n",
    "            alpha=1.0,\n",
    "            marker=\"o\", \n",
    "            label=\"True Source\"\n",
    "        )\n",
    "    if not nearby_vess is None:\n",
    "        nearby_vess.plot(\n",
    "            ax=ax, \n",
    "            color=\"none\",  # Ensure no fill color\n",
    "            edgecolor=\"none\",  # No edge color\n",
    "        )\n",
    "\n",
    "        # Add a red X marker using matplotlib\n",
    "        ax.scatter(\n",
    "            nearby_vess.geometry.x, \n",
    "            nearby_vess.geometry.y, \n",
    "            edgecolor='red',\n",
    "            color=\"yellow\",\n",
    "            alpha=0.5,\n",
    "            marker=\"x\", \n",
    "            label=\"Nearby Vessel\"\n",
    "        )\n",
    "\n",
    "    # Optionally, plot the centroid on top\n",
    "    centroid = analyzer.slick_gdf.centroid.iloc[0]\n",
    "    ax.plot(centroid.x, centroid.y, \"k+\", markersize=10, label=\"Centroid\")\n",
    "\n",
    "    # Set plot limits with padding\n",
    "    min_x, min_y, max_x, max_y = analyzer.slick_gdf.total_bounds\n",
    "    padding_ratio = padding_ratio\n",
    "\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "\n",
    "    padding_x = width * padding_ratio\n",
    "    padding_y = height * padding_ratio\n",
    "\n",
    "    # Apply padding\n",
    "    min_x_padded = min_x - padding_x\n",
    "    max_x_padded = max_x + padding_x\n",
    "    min_y_padded = min_y - padding_y\n",
    "    max_y_padded = max_y + padding_y\n",
    "\n",
    "    # Determine the larger dimension\n",
    "    width_padded = max_x_padded - min_x_padded\n",
    "    height_padded = max_y_padded - min_y_padded\n",
    "\n",
    "    if width_padded > height_padded:\n",
    "        # Width is the larger dimension\n",
    "        extra_height = width_padded - height_padded\n",
    "        min_y_final = min_y_padded - extra_height / 2\n",
    "        max_y_final = max_y_padded + extra_height / 2\n",
    "        min_x_final = min_x_padded\n",
    "        max_x_final = max_x_padded\n",
    "    else:\n",
    "        # Height is the larger dimension\n",
    "        extra_width = height_padded - width_padded\n",
    "        min_x_final = min_x_padded - extra_width / 2\n",
    "        max_x_final = max_x_padded + extra_width / 2\n",
    "        min_y_final = min_y_padded\n",
    "        max_y_final = max_y_padded\n",
    "\n",
    "    ax.set_xlim(min_x_final, max_x_final)\n",
    "    ax.set_ylim(min_y_final, max_y_final)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(\"Coincidence\")\n",
    "\n",
    "    max_coincidence = (\n",
    "        round(analyzer.coincidence_scores.max(), 2)\n",
    "        if len(analyzer.coincidence_scores)\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    if title is None:\n",
    "        plt.title(f\"Slick ID {slick_id}: Max Coincidence {max_coincidence}\")\n",
    "    else:\n",
    "        plt.title(f\"Slick ID {slick_id}: {title}\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # Remove or adjust the aspect ratio\n",
    "    # plt.axis(\"equal\")  # Removed to prevent overriding limits\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Optionally, add a legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        plt.legend(handles=handles, labels=labels)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def SLWBEAR(curve, slick_clean):\n",
    "    L = curve['length'].values\n",
    "    A = slick_clean['areas'].values\n",
    "    return np.sum(L**3 / A) / np.sum(L)\n",
    "\n",
    "def ARF(curve, slick_clean,ar_ref=16):\n",
    "    slwbear = SLWBEAR(curve,slick_clean)\n",
    "    return 1-math.exp((1-slwbear)/ar_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_scene(scene_id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a S1 scene GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.sentinel1_grd/items?scene_id={scene_id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{scene_id}.geojson\")\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for Scene {scene_id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "    s1_gdf = gpd.read_file(geojson_file_path)\n",
    "    s1_scene = SimpleNamespace(\n",
    "        scene_id=scene_id,\n",
    "        scihub_ingestion_time=s1_gdf.scihub_ingestion_time.iloc[0],\n",
    "        start_time=s1_gdf.start_time.iloc[0],\n",
    "        end_time=s1_gdf.end_time.iloc[0],\n",
    "        geometry=WKTElement(str(s1_gdf.geometry.iloc[0])),\n",
    "    )\n",
    "    return s1_scene\n",
    "\n",
    "def download_geojson(id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "\n",
    "    Parameters:\n",
    "    - id (int): The unique identifier for the GeoJSON item.\n",
    "    - download_path (str): The directory path where the GeoJSON will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - geojson_file_path (str): The file path to the downloaded GeoJSON.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.slick/items?id={id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{id}.geojson\")\n",
    "\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for ID {id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "\n",
    "    return geojson_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3_gdfs(gdf1,gdf2,gdf3):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    if not isinstance(gdf1,type(None)):\n",
    "        gdf1.plot(ax=ax, color=\"blue\", alpha=0.5, edgecolor=\"black\")\n",
    "    if not isinstance(gdf2,type(None)):\n",
    "        gdf2.plot(ax=ax, color=\"none\", edgecolor=\"red\", linestyle=\"--\")\n",
    "    if not isinstance(gdf3,type(None)):\n",
    "        gdf3.plot(ax=ax, color=\"green\", alpha=1.0, edgecolor=\"black\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Geometry with Bounding Boxes\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dark_vessel_coincidence_and_arf(scene_id, slick_id, sar_detections_gdf, \n",
    "                                    cutoff_radius=3000, theta_decay=0, \n",
    "                                    decay_factor=4.0, num_vertices=10, \n",
    "                                    ):\n",
    "\n",
    "    geojson_file_path = download_geojson(slick_id)\n",
    "    slick_gdf = gpd.read_file(geojson_file_path)\n",
    "    s1_scene = get_s1_scene(scene_id)\n",
    "    \n",
    "    dark_analyzer = DarkAnalyzer(\n",
    "        s1_scene, \n",
    "        dark_vessels_gdf=sar_detections_gdf, \n",
    "        cutoff_radius=cutoff_radius, \n",
    "        theta_decay=theta_decay,\n",
    "        decay_factor=decay_factor,\n",
    "        num_vertices=num_vertices,\n",
    "    )\n",
    "    return dark_analyzer.compute_coincidence_scores(slick_gdf), dark_analyzer.compute_ARF(slick_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_coin_with_groundtruth(coin, slick_id, dark_vess_gdf):\n",
    "    if coin is None:\n",
    "        return None\n",
    "    dark_vess = dark_vess_gdf[dark_vess_gdf['slick_id']==slick_id]\n",
    "    coin = coin.sort_values(by='coincidence_score',ascending=False)\n",
    "    coin['rank'] = list(range(1,len(coin)+1))\n",
    "    distances = coin.distance(dark_vess.geometry.iloc[0])\n",
    "    coin['truth'] = distances <= .005\n",
    "    return coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coin_on_true_vess_slicks(dark_vess_gdf, sar_detections_gdf,\n",
    "                                    cutoff_radius=3000, theta_decay=0, \n",
    "                                    decay_factor=4.0, num_vertices=10, \n",
    "                                     ):\n",
    "    false_vess_arf = []\n",
    "    false_vess_coin = []\n",
    "    true_vess_arf = []\n",
    "    true_vess_coin = [] \n",
    "\n",
    "    true_vess_rank = []\n",
    "    true_vess_mmsi = []\n",
    "\n",
    "    for i in tqdm(range(len(dark_vess_gdf))):\n",
    "        slick_id = dark_vess_gdf['slick_id'].values[i]\n",
    "        scene_id = dark_vess_gdf['scene_id'].values[i]\n",
    "\n",
    "        coin, arf = compute_dark_vessel_coincidence_and_arf(scene_id, slick_id, sar_detections_gdf,\n",
    "                                                            cutoff_radius=cutoff_radius, theta_decay=theta_decay, \n",
    "                                                            decay_factor=decay_factor, num_vertices=num_vertices, \n",
    "                                                            )\n",
    "        coin = label_coin_with_groundtruth(coin, slick_id, dark_vess_gdf)\n",
    "\n",
    "        if coin is None:\n",
    "            continue\n",
    "\n",
    "        for _,c in coin.iterrows():\n",
    "            if c['truth']:\n",
    "                true_vess_arf.append(arf)\n",
    "                true_vess_coin.append(c['coincidence_score'])\n",
    "                true_vess_rank.append(c['rank'])\n",
    "                true_vess_mmsi.append(c['ssvid'])\n",
    "            else:\n",
    "                false_vess_arf.append(arf)\n",
    "                false_vess_coin.append(c['coincidence_score'])\n",
    "        clear_output()\n",
    "    return false_vess_arf, false_vess_coin, true_vess_arf, true_vess_coin , true_vess_rank, true_vess_mmsi\n",
    "\n",
    "def compute_coin_on_false_positive_slicks(reviewed_fp, fp_sar_detections_gdf,\n",
    "                                            cutoff_radius=3000, theta_decay=0, \n",
    "                                            decay_factor=4.0, num_vertices=10, \n",
    "                                          ):\n",
    "    fp_arf = []\n",
    "    fp_coin = []\n",
    "    for i in tqdm(range(len(reviewed_fp))):\n",
    "        slick_id = reviewed_fp['Slick ID'].values[i]\n",
    "        scene_id = reviewed_fp['Scene ID'].values[i]\n",
    "        coin, arf = compute_dark_vessel_coincidence_and_arf(scene_id, slick_id, fp_sar_detections_gdf,\n",
    "                                                            cutoff_radius=cutoff_radius, theta_decay=theta_decay, \n",
    "                                                            decay_factor=decay_factor, num_vertices=num_vertices, \n",
    "                                                            )\n",
    "        if coin is None:\n",
    "            continue\n",
    "        for _,c in coin.iterrows():\n",
    "            fp_arf.append(arf)\n",
    "            fp_coin.append(c['coincidence_score'])\n",
    "        clear_output()\n",
    "    return fp_arf, fp_coin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dark Vessel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_vess_df = pd.read_csv(r'refined_dark_vessel_dataset.csv').drop(columns='Unnamed: 0').drop(columns='index')\n",
    "sar_detections = pd.read_csv(r'sar_detections_hitl_dark_ds.csv').drop(columns='Unnamed: 0').drop(columns='index')\n",
    "\n",
    "true_dark_vess_df = dark_vess_df[dark_vess_df['mmsi'].isna()]\n",
    "\n",
    "#Toggle this on and off to incorporate long distance coincidence (vessels ~20km or more away from slick)\n",
    "long_distance_coincidence = [3581643, 3581482, 3581103, 3581287, 3581532, 3581538, 3582446, 3581900, 3582053, 3580996, 3581711, 3581920, 3581075, 3581141, 3581812, 3582235, 3582465, 3582774, 3582584]\n",
    "dark_vess_df = dark_vess_df[[not (slick_id in long_distance_coincidence) for slick_id in dark_vess_df['slick_id'].values]]\n",
    "\n",
    "dark_vess_gdf = gpd.GeoDataFrame(\n",
    "    dark_vess_df, \n",
    "    geometry=gpd.points_from_xy(dark_vess_df['lon'], dark_vess_df['lat']),  # Create geometry column\n",
    "    crs=\"EPSG:4326\"  # Set CRS to WGS 84\n",
    ")\n",
    "\n",
    "sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    sar_detections, \n",
    "    geometry=gpd.points_from_xy(sar_detections['detect_lon'], sar_detections['detect_lat']),  # Create geometry column\n",
    "    crs=\"EPSG:4326\"  # Set CRS to WGS 84\n",
    ")\n",
    "\n",
    "sar_detections_gdf = sar_detections_gdf[sar_detections_gdf['structure_id'].isna()]\n",
    "sar_detections_gdf = sar_detections_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load False Positives data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_fp = pd.read_csv(r'HITL Source Dataset ASA - False Positives.csv')\n",
    "reviewed_fp = reviewed_fp[reviewed_fp['Marked As False']=='y']\n",
    "fp_sar_ds = pd.read_csv('false_positives_sar_detections.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "fp_sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    fp_sar_ds, \n",
    "    geometry=gpd.points_from_xy(fp_sar_ds['detect_lon'], fp_sar_ds['detect_lat']),  # Create geometry column\n",
    "    crs=\"EPSG:4326\"  # Set CRS to WGS 84\n",
    ")\n",
    "\n",
    "fp_sar_detections_gdf = fp_sar_detections_gdf[fp_sar_detections_gdf['structure_id'].isna()]\n",
    "fp_sar_detections_gdf = fp_sar_detections_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select parameters and compute ARF and coincidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_radius=20000\n",
    "theta_decay=2.0\n",
    "decay_factor=4.0\n",
    "num_vertices=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_arf, fp_coin = compute_coin_on_false_positive_slicks(reviewed_fp, fp_sar_detections_gdf,\n",
    "                                                        cutoff_radius=cutoff_radius,\n",
    "                                                        theta_decay=theta_decay,\n",
    "                                                        decay_factor=decay_factor,\n",
    "                                                        num_vertices=num_vertices,\n",
    "                                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_vess_arf, false_vess_coin, true_vess_arf, true_vess_coin , true_vess_rank, true_vess_mmsi = compute_coin_on_true_vess_slicks(dark_vess_gdf, sar_detections_gdf,                                                                                                                                                        \n",
    "                                                                                                  cutoff_radius=cutoff_radius,\n",
    "                                                                                                  theta_decay=theta_decay,\n",
    "                                                                                                  decay_factor=decay_factor,\n",
    "                                                                                                  num_vertices=num_vertices,\n",
    "                                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit Rate Metrics for ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(100*round(len(true_vess_coin)/len(dark_vess_gdf),5))+\"%\", f\"of groundtruth found at {cutoff_radius} meter radius of interest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_rate = sum(np.array(true_vess_rank)<=1)/len(dark_vess_gdf)\n",
    "top_3_rate = sum(np.array(true_vess_rank)<=3)/len(dark_vess_gdf)\n",
    "print(str(100*round(top_1_rate, 5))+\"%\", f\"top 1 source rate at {cutoff_radius} meter radius of interest\")\n",
    "print(str(100*round(top_3_rate, 5))+\"%\", f\"top 3 source rate at {cutoff_radius} meter radius of interest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coincidence Score and ARF for true versus false slick attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for different categories\n",
    "plt.scatter(false_vess_coin, false_vess_arf, color='yellow', label='False Attributions to Dark Vess Slicks')\n",
    "plt.scatter(true_vess_coin, true_vess_arf, color='green', label='True Attributions to Dark Vess Slicks')\n",
    "plt.scatter(fp_coin, fp_arf, color='red', label='False Attributions to False Positive Slicks')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Coin\")\n",
    "plt.ylabel(\"ARF\")\n",
    "plt.title(\"True and False Dark Vessel Attributions\" + f\" - {cutoff_radius}m radius\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dark vessel slicks with distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_id = random.randint(0,len(dark_vess_gdf))\n",
    "# ex_id = 3\n",
    "dark_vess = dark_vess_gdf.iloc[[ex_id]].reset_index()\n",
    "slick_id = dark_vess['slick_id'].values[0]\n",
    "s1_scene_id = dark_vess['scene_id'].values[0]\n",
    "geojson_file_path = download_geojson(slick_id)\n",
    "slick_gdf = gpd.read_file(geojson_file_path)\n",
    "s1_scene = get_s1_scene(s1_scene_id)\n",
    "fake_infra_gdf = generate_infrastructure_points(slick_gdf, 50000, expansion_factor=1.0)\n",
    "dist_analyzer = DarkAnalyzer(s1_scene, dark_vessels_gdf = fake_infra_gdf, cutoff_radius=cutoff_radius, num_vertices=num_vertices, decay_factor=decay_factor,theta_decay=theta_decay)\n",
    "true_analyzer = DarkAnalyzer(s1_scene, dark_vessels_gdf = dark_vess, cutoff_radius=cutoff_radius, num_vertices=num_vertices, decay_factor=decay_factor,theta_decay=theta_decay)\n",
    "coincidence_scores = dist_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "truth = true_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "if truth is None:\n",
    "    true_coin = 0.0\n",
    "else:\n",
    "    true_coin = truth['coincidence_score'].values[0] if len(truth)>0 else 0.0\n",
    "clear_output()\n",
    "print(slick_id)\n",
    "plot_coincidence(dist_analyzer, slick_id, False, true_point=dark_vess, title = f\" Radius {cutoff_radius} meters - True Coincidence Score {round(true_coin,3)}\", padding_ratio=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View false positives with distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_id = random.randint(0,len(reviewed_fp))\n",
    "fp = reviewed_fp.iloc[[ex_id]].reset_index()\n",
    "slick_id = fp['Slick ID'].values[0]\n",
    "s1_scene_id = fp['Scene ID'].values[0]\n",
    "geojson_file_path = download_geojson(slick_id)\n",
    "slick_gdf = gpd.read_file(geojson_file_path)\n",
    "s1_scene = get_s1_scene(s1_scene_id)\n",
    "fake_infra_gdf = generate_infrastructure_points(slick_gdf, 50000, expansion_factor=1.0)\n",
    "dist_analyzer = DarkAnalyzer(s1_scene, dark_vessels_gdf = fake_infra_gdf, cutoff_radius=cutoff_radius, num_vertices=num_vertices, decay_factor=decay_factor, theta_decay=theta_decay)\n",
    "nearby_analyzer = DarkAnalyzer(s1_scene, dark_vessels_gdf = fp_sar_detections_gdf, cutoff_radius=cutoff_radius, num_vertices=num_vertices, decay_factor=decay_factor, theta_decay=theta_decay)\n",
    "coincidence_scores = dist_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "nearby = nearby_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "nearby_coin = 0.0\n",
    "\n",
    "if not nearby is None:\n",
    "    pass\n",
    "    # nearby = nearby[nearby['ssvid'].isna()].reset_index() #toggle AIS off and on\n",
    "    if len(nearby) == 0:\n",
    "        nearby=None\n",
    "    else:\n",
    "        nearby_coin = nearby.sort_values(by='coincidence_score', ascending=False).iloc[0]['coincidence_score']\n",
    "\n",
    "clear_output()\n",
    "plot_coincidence(dist_analyzer, slick_id, False, true_point=None, nearby_vess = nearby, title = f\" Radius {cutoff_radius} meters, highest coin score - {round(nearby_coin,3)}\", padding_ratio=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
