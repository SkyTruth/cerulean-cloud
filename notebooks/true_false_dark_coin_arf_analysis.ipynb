{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from geoalchemy2 import WKTElement\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\ebeva\\SkyTruth\\git\\cerulean-cloud\")\n",
    "load_dotenv(r\"C:\\Users\\ebeva\\.env\")\n",
    "\n",
    "from cerulean_cloud.cloud_function_ais_analysis.utils.analyzer import (  # noqa: E402\n",
    "    DarkAnalyzer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_infrastructure_points(\n",
    "    slick_gdf, num_points, expansion_factor=0.2, crs=\"epsg:4326\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates random infrastructure points within an expanded bounding box of the combined geometry.\n",
    "\n",
    "    Parameters:\n",
    "    - slick_gdf (GeoDataFrame): GeoDataFrame containing slick polygons.\n",
    "    - num_points (int): Number of infrastructure points to generate.\n",
    "    - expansion_factor (float): Fraction to expand the bounding box.\n",
    "\n",
    "    Returns:\n",
    "    - infra_gdf (GeoDataFrame): GeoDataFrame of infrastructure points.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = slick_gdf.total_bounds\n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "    infra_x = np.random.uniform(\n",
    "        minx - expansion_factor * width, maxx + expansion_factor * width, num_points\n",
    "    )\n",
    "    infra_y = np.random.uniform(\n",
    "        miny - expansion_factor * height, maxy + expansion_factor * height, num_points\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"structure_start_date\": [pd.Timestamp(0)] * num_points,\n",
    "            \"structure_end_date\": [pd.Timestamp.now()] * num_points,\n",
    "        }\n",
    "    )\n",
    "    infra_gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(infra_x, infra_y), crs=crs\n",
    "    )\n",
    "    return infra_gdf\n",
    "\n",
    "\n",
    "def plot_coincidence(\n",
    "    analyzer,\n",
    "    slick_id,\n",
    "    black=True,\n",
    "    true_point=None,\n",
    "    nearby_vess=None,\n",
    "    padding_ratio=0.2,\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a sample of infrastructure points with their coincidence scores.\n",
    "\n",
    "    Parameters:\n",
    "    - analyzer (SourceAnalyzer): Analyzer object containing infrastructure points and coincidence scores.\n",
    "    - slick_id (int): Identifier for the plot title.\n",
    "    - black (bool): Whether to use black borders for the infrastructure points.\n",
    "    \"\"\"\n",
    "\n",
    "    sample_size = len(analyzer.infra_gdf)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Create an axes object\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # First plot the infrastructure points\n",
    "    scatter = ax.scatter(\n",
    "        analyzer.infra_gdf.geometry.x[:sample_size],\n",
    "        analyzer.infra_gdf.geometry.y[:sample_size],\n",
    "        c=analyzer.coincidence_scores[:sample_size],\n",
    "        cmap=\"Blues\",\n",
    "        s=10,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        # alpha=analyzer.coincidence_scores[:sample_size],\n",
    "        edgecolor=\"black\" if black else None,  # Adds black borders\n",
    "        # linewidth=0.5,  # Optional: adjust border thickness\n",
    "        label=\"Coincidence Score Distribution\",\n",
    "    )\n",
    "\n",
    "    # Then plot the slick_gdf polygons on top\n",
    "    analyzer.slick_gdf.plot(\n",
    "        edgecolor=\"red\", linewidth=1, color=\"none\", ax=ax, label=\"Slick Polygons\"\n",
    "    )\n",
    "\n",
    "    if true_point is not None:\n",
    "        true_point.plot(\n",
    "            ax=ax,\n",
    "            color=\"none\",  # Ensure no fill color\n",
    "            edgecolor=\"none\",  # No edge color\n",
    "        )\n",
    "\n",
    "        # Add a green X marker using matplotlib\n",
    "        ax.scatter(\n",
    "            true_point.geometry.x,\n",
    "            true_point.geometry.y,\n",
    "            edgecolor=\"green\",\n",
    "            color=\"none\",\n",
    "            alpha=1.0,\n",
    "            marker=\"o\",\n",
    "            label=\"True Source\",\n",
    "        )\n",
    "    if nearby_vess is not None:\n",
    "        nearby_vess.plot(\n",
    "            ax=ax,\n",
    "            color=\"none\",  # Ensure no fill color\n",
    "            edgecolor=\"none\",  # No edge color\n",
    "        )\n",
    "\n",
    "        # Add a red X marker using matplotlib\n",
    "        ax.scatter(\n",
    "            nearby_vess.geometry.x,\n",
    "            nearby_vess.geometry.y,\n",
    "            edgecolor=\"red\",\n",
    "            color=\"yellow\",\n",
    "            alpha=0.5,\n",
    "            marker=\"x\",\n",
    "            label=\"Nearby Vessel\",\n",
    "        )\n",
    "\n",
    "    # Optionally, plot the centroid on top\n",
    "    centroid = analyzer.slick_gdf.centroid.iloc[0]\n",
    "    ax.plot(centroid.x, centroid.y, \"k+\", markersize=10, label=\"Centroid\")\n",
    "\n",
    "    # Set plot limits with padding\n",
    "    min_x, min_y, max_x, max_y = analyzer.slick_gdf.total_bounds\n",
    "    padding_ratio = padding_ratio\n",
    "\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "\n",
    "    padding_x = width * padding_ratio\n",
    "    padding_y = height * padding_ratio\n",
    "\n",
    "    # Apply padding\n",
    "    min_x_padded = min_x - padding_x\n",
    "    max_x_padded = max_x + padding_x\n",
    "    min_y_padded = min_y - padding_y\n",
    "    max_y_padded = max_y + padding_y\n",
    "\n",
    "    # Determine the larger dimension\n",
    "    width_padded = max_x_padded - min_x_padded\n",
    "    height_padded = max_y_padded - min_y_padded\n",
    "\n",
    "    if width_padded > height_padded:\n",
    "        # Width is the larger dimension\n",
    "        extra_height = width_padded - height_padded\n",
    "        min_y_final = min_y_padded - extra_height / 2\n",
    "        max_y_final = max_y_padded + extra_height / 2\n",
    "        min_x_final = min_x_padded\n",
    "        max_x_final = max_x_padded\n",
    "    else:\n",
    "        # Height is the larger dimension\n",
    "        extra_width = height_padded - width_padded\n",
    "        min_x_final = min_x_padded - extra_width / 2\n",
    "        max_x_final = max_x_padded + extra_width / 2\n",
    "        min_y_final = min_y_padded\n",
    "        max_y_final = max_y_padded\n",
    "\n",
    "    ax.set_xlim(min_x_final, max_x_final)\n",
    "    ax.set_ylim(min_y_final, max_y_final)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(\"Coincidence\")\n",
    "\n",
    "    max_coincidence = (\n",
    "        round(analyzer.coincidence_scores.max(), 2)\n",
    "        if len(analyzer.coincidence_scores)\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    if title is None:\n",
    "        plt.title(f\"Slick ID {slick_id}: Max Coincidence {max_coincidence}\")\n",
    "    else:\n",
    "        plt.title(f\"Slick ID {slick_id}: {title}\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # Remove or adjust the aspect ratio\n",
    "    # plt.axis(\"equal\")  # Removed to prevent overriding limits\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Optionally, add a legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        plt.legend(handles=handles, labels=labels)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLWBEAR(curve, slick_clean):\n",
    "    L = curve[\"length\"].values\n",
    "    A = slick_clean[\"areas\"].values\n",
    "    return np.sum(L**3 / A) / np.sum(L)\n",
    "\n",
    "\n",
    "def ARF(curve, slick_clean, ar_ref=16):\n",
    "    slwbear = SLWBEAR(curve, slick_clean)\n",
    "    return 1 - math.exp((1 - slwbear) / ar_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_scene(scene_id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a S1 scene GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.sentinel1_grd/items?scene_id={scene_id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{scene_id}.geojson\")\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for Scene {scene_id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "    s1_gdf = gpd.read_file(geojson_file_path)\n",
    "    s1_scene = SimpleNamespace(\n",
    "        scene_id=scene_id,\n",
    "        scihub_ingestion_time=s1_gdf.scihub_ingestion_time.iloc[0],\n",
    "        start_time=s1_gdf.start_time.iloc[0],\n",
    "        end_time=s1_gdf.end_time.iloc[0],\n",
    "        geometry=WKTElement(str(s1_gdf.geometry.iloc[0])),\n",
    "    )\n",
    "    return s1_scene\n",
    "\n",
    "\n",
    "def download_geojson(id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "\n",
    "    Parameters:\n",
    "    - id (int): The unique identifier for the GeoJSON item.\n",
    "    - download_path (str): The directory path where the GeoJSON will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - geojson_file_path (str): The file path to the downloaded GeoJSON.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.slick/items?id={id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{id}.geojson\")\n",
    "\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for ID {id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "\n",
    "    return geojson_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3_gdfs(gdf1, gdf2, gdf3):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    if not isinstance(gdf1, type(None)):\n",
    "        gdf1.plot(ax=ax, color=\"blue\", alpha=0.5, edgecolor=\"black\")\n",
    "    if not isinstance(gdf2, type(None)):\n",
    "        gdf2.plot(ax=ax, color=\"none\", edgecolor=\"red\", linestyle=\"--\")\n",
    "    if not isinstance(gdf3, type(None)):\n",
    "        gdf3.plot(ax=ax, color=\"green\", alpha=1.0, edgecolor=\"black\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Geometry with Bounding Boxes\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dark_vessel_coincidence_and_arf(\n",
    "    scene_id,\n",
    "    slick_id,\n",
    "    sar_detections_gdf,\n",
    "    cutoff_radius=3000,\n",
    "    theta_decay=0,\n",
    "    decay_factor=4.0,\n",
    "    num_vertices=10,\n",
    "):\n",
    "    geojson_file_path = download_geojson(slick_id)\n",
    "    slick_gdf = gpd.read_file(geojson_file_path)\n",
    "    s1_scene = get_s1_scene(scene_id)\n",
    "\n",
    "    dark_analyzer = DarkAnalyzer(\n",
    "        s1_scene,\n",
    "        dark_vessels_gdf=sar_detections_gdf,\n",
    "        cutoff_radius=cutoff_radius,\n",
    "        theta_decay=theta_decay,\n",
    "        decay_factor=decay_factor,\n",
    "        num_vertices=num_vertices,\n",
    "    )\n",
    "    return dark_analyzer.compute_coincidence_scores(\n",
    "        slick_gdf\n",
    "    ), dark_analyzer.compute_ARF(slick_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_coin_with_groundtruth(coin, slick_id, dark_vess_gdf):\n",
    "    if coin is None:\n",
    "        return None\n",
    "    dark_vess = dark_vess_gdf[dark_vess_gdf[\"slick_id\"] == slick_id]\n",
    "    coin = coin.sort_values(by=\"coincidence_score\", ascending=False)\n",
    "    coin[\"rank\"] = list(range(1, len(coin) + 1))\n",
    "    distances = coin.distance(dark_vess.geometry.iloc[0])\n",
    "    coin[\"truth\"] = distances <= 0.005\n",
    "    return coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coin_on_true_vess_slicks(\n",
    "    dark_vess_gdf,\n",
    "    sar_detections_gdf,\n",
    "    cutoff_radius=3000,\n",
    "    theta_decay=0,\n",
    "    decay_factor=4.0,\n",
    "    num_vertices=10,\n",
    "):\n",
    "    false_vess_arf = []\n",
    "    false_vess_coin = []\n",
    "    true_vess_arf = []\n",
    "    true_vess_coin = []\n",
    "\n",
    "    true_vess_rank = []\n",
    "    true_vess_mmsi = []\n",
    "\n",
    "    for i in tqdm(range(len(dark_vess_gdf))):\n",
    "        slick_id = dark_vess_gdf[\"slick_id\"].values[i]\n",
    "        scene_id = dark_vess_gdf[\"scene_id\"].values[i]\n",
    "\n",
    "        coin, arf = compute_dark_vessel_coincidence_and_arf(\n",
    "            scene_id,\n",
    "            slick_id,\n",
    "            sar_detections_gdf,\n",
    "            cutoff_radius=cutoff_radius,\n",
    "            theta_decay=theta_decay,\n",
    "            decay_factor=decay_factor,\n",
    "            num_vertices=num_vertices,\n",
    "        )\n",
    "        coin = label_coin_with_groundtruth(coin, slick_id, dark_vess_gdf)\n",
    "\n",
    "        if coin is None:\n",
    "            continue\n",
    "\n",
    "        for _, c in coin.iterrows():\n",
    "            if c[\"truth\"]:\n",
    "                true_vess_arf.append(arf)\n",
    "                true_vess_coin.append(c[\"coincidence_score\"])\n",
    "                true_vess_rank.append(c[\"rank\"])\n",
    "                true_vess_mmsi.append(c[\"ssvid\"])\n",
    "            else:\n",
    "                false_vess_arf.append(arf)\n",
    "                false_vess_coin.append(c[\"coincidence_score\"])\n",
    "        clear_output()\n",
    "    return (\n",
    "        false_vess_arf,\n",
    "        false_vess_coin,\n",
    "        true_vess_arf,\n",
    "        true_vess_coin,\n",
    "        true_vess_rank,\n",
    "        true_vess_mmsi,\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_coin_on_false_positive_slicks(\n",
    "    reviewed_fp,\n",
    "    fp_sar_detections_gdf,\n",
    "    cutoff_radius=3000,\n",
    "    theta_decay=0,\n",
    "    decay_factor=4.0,\n",
    "    num_vertices=10,\n",
    "):\n",
    "    fp_arf = []\n",
    "    fp_coin = []\n",
    "    for i in tqdm(range(len(reviewed_fp))):\n",
    "        slick_id = reviewed_fp[\"Slick ID\"].values[i]\n",
    "        scene_id = reviewed_fp[\"Scene ID\"].values[i]\n",
    "        coin, arf = compute_dark_vessel_coincidence_and_arf(\n",
    "            scene_id,\n",
    "            slick_id,\n",
    "            fp_sar_detections_gdf,\n",
    "            cutoff_radius=cutoff_radius,\n",
    "            theta_decay=theta_decay,\n",
    "            decay_factor=decay_factor,\n",
    "            num_vertices=num_vertices,\n",
    "        )\n",
    "        if coin is None:\n",
    "            continue\n",
    "        for _, c in coin.iterrows():\n",
    "            fp_arf.append(arf)\n",
    "            fp_coin.append(c[\"coincidence_score\"])\n",
    "        clear_output()\n",
    "    return fp_arf, fp_coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dark Vessel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_vess_df = (\n",
    "    pd.read_csv(r\"refined_dark_vessel_dataset.csv\")\n",
    "    .drop(columns=\"Unnamed: 0\")\n",
    "    .drop(columns=\"index\")\n",
    ")\n",
    "sar_detections = (\n",
    "    pd.read_csv(r\"sar_detections_hitl_dark_ds.csv\")\n",
    "    .drop(columns=\"Unnamed: 0\")\n",
    "    .drop(columns=\"index\")\n",
    ")\n",
    "\n",
    "true_dark_vess_df = dark_vess_df[dark_vess_df[\"mmsi\"].isna()]\n",
    "\n",
    "# Toggle this on and off to incorporate long distance coincidence (vessels ~20km or more away from slick)\n",
    "long_distance_coincidence = [\n",
    "    3581643,\n",
    "    3581482,\n",
    "    3581103,\n",
    "    3581287,\n",
    "    3581532,\n",
    "    3581538,\n",
    "    3582446,\n",
    "    3581900,\n",
    "    3582053,\n",
    "    3580996,\n",
    "    3581711,\n",
    "    3581920,\n",
    "    3581075,\n",
    "    3581141,\n",
    "    3581812,\n",
    "    3582235,\n",
    "    3582465,\n",
    "    3582774,\n",
    "    3582584,\n",
    "]\n",
    "dark_vess_df = dark_vess_df[\n",
    "    [\n",
    "        slick_id not in long_distance_coincidence\n",
    "        for slick_id in dark_vess_df[\"slick_id\"].values\n",
    "    ]\n",
    "]\n",
    "\n",
    "dark_vess_gdf = gpd.GeoDataFrame(\n",
    "    dark_vess_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        dark_vess_df[\"lon\"], dark_vess_df[\"lat\"]\n",
    "    ),  # Create geometry column\n",
    "    crs=\"EPSG:4326\",  # Set CRS to WGS 84\n",
    ")\n",
    "\n",
    "sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    sar_detections,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        sar_detections[\"detect_lon\"], sar_detections[\"detect_lat\"]\n",
    "    ),  # Create geometry column\n",
    "    crs=\"EPSG:4326\",  # Set CRS to WGS 84\n",
    ")\n",
    "\n",
    "sar_detections_gdf = sar_detections_gdf[sar_detections_gdf[\"structure_id\"].isna()]\n",
    "sar_detections_gdf = sar_detections_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load False Positives data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_fp = pd.read_csv(r\"HITL Source Dataset ASA - False Positives.csv\")\n",
    "reviewed_fp = reviewed_fp[reviewed_fp[\"Marked As False\"] == \"y\"]\n",
    "fp_sar_ds = pd.read_csv(\"false_positives_sar_detections.csv\").drop(\n",
    "    columns=[\"Unnamed: 0\"]\n",
    ")\n",
    "\n",
    "fp_sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    fp_sar_ds,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        fp_sar_ds[\"detect_lon\"], fp_sar_ds[\"detect_lat\"]\n",
    "    ),  # Create geometry column\n",
    "    crs=\"EPSG:4326\",  # Set CRS to WGS 84\n",
    ")\n",
    "\n",
    "fp_sar_detections_gdf = fp_sar_detections_gdf[\n",
    "    fp_sar_detections_gdf[\"structure_id\"].isna()\n",
    "]\n",
    "fp_sar_detections_gdf = fp_sar_detections_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select parameters and compute ARF and coincidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_radius = 20000\n",
    "theta_decay = 2.0\n",
    "decay_factor = 4.0\n",
    "num_vertices = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_arf, fp_coin = compute_coin_on_false_positive_slicks(\n",
    "    reviewed_fp,\n",
    "    fp_sar_detections_gdf,\n",
    "    cutoff_radius=cutoff_radius,\n",
    "    theta_decay=theta_decay,\n",
    "    decay_factor=decay_factor,\n",
    "    num_vertices=num_vertices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    false_vess_arf,\n",
    "    false_vess_coin,\n",
    "    true_vess_arf,\n",
    "    true_vess_coin,\n",
    "    true_vess_rank,\n",
    "    true_vess_mmsi,\n",
    ") = compute_coin_on_true_vess_slicks(\n",
    "    dark_vess_gdf,\n",
    "    sar_detections_gdf,\n",
    "    cutoff_radius=cutoff_radius,\n",
    "    theta_decay=theta_decay,\n",
    "    decay_factor=decay_factor,\n",
    "    num_vertices=num_vertices,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit Rate Metrics for ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    str(100 * round(len(true_vess_coin) / len(dark_vess_gdf), 5)) + \"%\",\n",
    "    f\"of groundtruth found at {cutoff_radius} meter radius of interest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_rate = sum(np.array(true_vess_rank) <= 1) / len(dark_vess_gdf)\n",
    "top_3_rate = sum(np.array(true_vess_rank) <= 3) / len(dark_vess_gdf)\n",
    "print(\n",
    "    str(100 * round(top_1_rate, 5)) + \"%\",\n",
    "    f\"top 1 source rate at {cutoff_radius} meter radius of interest\",\n",
    ")\n",
    "print(\n",
    "    str(100 * round(top_3_rate, 5)) + \"%\",\n",
    "    f\"top 3 source rate at {cutoff_radius} meter radius of interest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coincidence Score and ARF for true versus false slick attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for different categories\n",
    "plt.scatter(\n",
    "    false_vess_coin,\n",
    "    false_vess_arf,\n",
    "    color=\"yellow\",\n",
    "    label=\"False Attributions to Dark Vess Slicks\",\n",
    ")\n",
    "plt.scatter(\n",
    "    true_vess_coin,\n",
    "    true_vess_arf,\n",
    "    color=\"green\",\n",
    "    label=\"True Attributions to Dark Vess Slicks\",\n",
    ")\n",
    "plt.scatter(\n",
    "    fp_coin, fp_arf, color=\"red\", label=\"False Attributions to False Positive Slicks\"\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Coin\")\n",
    "plt.ylabel(\"ARF\")\n",
    "plt.title(\"True and False Dark Vessel Attributions\" + f\" - {cutoff_radius}m radius\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dark vessel slicks with distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_id = random.randint(0, len(dark_vess_gdf))\n",
    "# ex_id = 3\n",
    "dark_vess = dark_vess_gdf.iloc[[ex_id]].reset_index()\n",
    "slick_id = dark_vess[\"slick_id\"].values[0]\n",
    "s1_scene_id = dark_vess[\"scene_id\"].values[0]\n",
    "geojson_file_path = download_geojson(slick_id)\n",
    "slick_gdf = gpd.read_file(geojson_file_path)\n",
    "s1_scene = get_s1_scene(s1_scene_id)\n",
    "fake_infra_gdf = generate_infrastructure_points(slick_gdf, 50000, expansion_factor=1.0)\n",
    "dist_analyzer = DarkAnalyzer(\n",
    "    s1_scene,\n",
    "    dark_vessels_gdf=fake_infra_gdf,\n",
    "    cutoff_radius=cutoff_radius,\n",
    "    num_vertices=num_vertices,\n",
    "    decay_factor=decay_factor,\n",
    "    theta_decay=theta_decay,\n",
    ")\n",
    "true_analyzer = DarkAnalyzer(\n",
    "    s1_scene,\n",
    "    dark_vessels_gdf=dark_vess,\n",
    "    cutoff_radius=cutoff_radius,\n",
    "    num_vertices=num_vertices,\n",
    "    decay_factor=decay_factor,\n",
    "    theta_decay=theta_decay,\n",
    ")\n",
    "coincidence_scores = dist_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "truth = true_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "if truth is None:\n",
    "    true_coin = 0.0\n",
    "else:\n",
    "    true_coin = truth[\"coincidence_score\"].values[0] if len(truth) > 0 else 0.0\n",
    "clear_output()\n",
    "print(slick_id)\n",
    "plot_coincidence(\n",
    "    dist_analyzer,\n",
    "    slick_id,\n",
    "    False,\n",
    "    true_point=dark_vess,\n",
    "    title=f\" Radius {cutoff_radius} meters - True Coincidence Score {round(true_coin, 3)}\",\n",
    "    padding_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View false positives with distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_id = random.randint(0, len(reviewed_fp))\n",
    "fp = reviewed_fp.iloc[[ex_id]].reset_index()\n",
    "slick_id = fp[\"Slick ID\"].values[0]\n",
    "s1_scene_id = fp[\"Scene ID\"].values[0]\n",
    "geojson_file_path = download_geojson(slick_id)\n",
    "slick_gdf = gpd.read_file(geojson_file_path)\n",
    "s1_scene = get_s1_scene(s1_scene_id)\n",
    "fake_infra_gdf = generate_infrastructure_points(slick_gdf, 50000, expansion_factor=1.0)\n",
    "dist_analyzer = DarkAnalyzer(\n",
    "    s1_scene,\n",
    "    dark_vessels_gdf=fake_infra_gdf,\n",
    "    cutoff_radius=cutoff_radius,\n",
    "    num_vertices=num_vertices,\n",
    "    decay_factor=decay_factor,\n",
    "    theta_decay=theta_decay,\n",
    ")\n",
    "nearby_analyzer = DarkAnalyzer(\n",
    "    s1_scene,\n",
    "    dark_vessels_gdf=fp_sar_detections_gdf,\n",
    "    cutoff_radius=cutoff_radius,\n",
    "    num_vertices=num_vertices,\n",
    "    decay_factor=decay_factor,\n",
    "    theta_decay=theta_decay,\n",
    ")\n",
    "coincidence_scores = dist_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "nearby = nearby_analyzer.compute_coincidence_scores(slick_gdf)\n",
    "nearby_coin = 0.0\n",
    "\n",
    "if nearby is not None:\n",
    "    pass\n",
    "    # nearby = nearby[nearby['ssvid'].isna()].reset_index() #toggle AIS off and on\n",
    "    if len(nearby) == 0:\n",
    "        nearby = None\n",
    "    else:\n",
    "        nearby_coin = nearby.sort_values(by=\"coincidence_score\", ascending=False).iloc[\n",
    "            0\n",
    "        ][\"coincidence_score\"]\n",
    "\n",
    "clear_output()\n",
    "plot_coincidence(\n",
    "    dist_analyzer,\n",
    "    slick_id,\n",
    "    False,\n",
    "    true_point=None,\n",
    "    nearby_vess=nearby,\n",
    "    title=f\" Radius {cutoff_radius} meters, highest coin score - {round(nearby_coin, 3)}\",\n",
    "    padding_ratio=1.0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
