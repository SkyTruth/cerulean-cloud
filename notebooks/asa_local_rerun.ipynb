{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate ASA outputs locally and save results as a dataframe. Test ASA on adjusted values in constants.py or analyzer.py changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from geoalchemy2 import WKTElement\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_path = os.getenv(\"GIT_FOLDER\")\n",
    "cv3_path = os.getenv(\"CV3_FOLDER\") \n",
    "sys.path.append(git_path)\n",
    "sys.path.append(cv3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerulean_cloud.cloud_function_asa.utils.analyzer import (  # noqa: E402\n",
    "    ASA_MAPPING,\n",
    "    InfrastructureAnalyzer,\n",
    "    AISAnalyzer,\n",
    "    DarkAnalyzer,\n",
    "    SourceAnalyzer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_scene(scene_id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a S1 scene GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.sentinel1_grd/items?scene_id={scene_id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{scene_id}.geojson\")\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for Scene {scene_id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "    s1_gdf = gpd.read_file(geojson_file_path)\n",
    "    s1_scene = SimpleNamespace(\n",
    "        scene_id=scene_id,\n",
    "        scihub_ingestion_time=s1_gdf.scihub_ingestion_time.iloc[0],\n",
    "        start_time=s1_gdf.start_time.iloc[0],\n",
    "        end_time=s1_gdf.end_time.iloc[0],\n",
    "        geometry=WKTElement(str(s1_gdf.geometry.iloc[0])),\n",
    "    )\n",
    "    return s1_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_geojson(id, download_path=os.getenv(\"ASA_DOWNLOAD_PATH\")):\n",
    "    \"\"\"\n",
    "    Downloads a GeoJSON file from the specified URL if it hasn't been downloaded already.\n",
    "\n",
    "    Parameters:\n",
    "    - id (int): The unique identifier for the GeoJSON item.\n",
    "    - download_path (str): The directory path where the GeoJSON will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - geojson_file_path (str): The file path to the downloaded GeoJSON.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.cerulean.skytruth.org/collections/public.slick/items?id={id}&f=geojson\"\n",
    "    geojson_file_path = os.path.join(download_path, f\"{id}.geojson\")\n",
    "\n",
    "    if not os.path.exists(geojson_file_path):\n",
    "        print(f\"Downloading GeoJSON file for ID {id}...\")\n",
    "        os.system(f'curl \"{url}\" -o \"{geojson_file_path}\"')\n",
    "        print(f\"Downloaded GeoJSON to {geojson_file_path}\")\n",
    "    else:\n",
    "        print(f\"GeoJSON file already exists at {geojson_file_path}. Skipping download.\")\n",
    "\n",
    "    return geojson_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataframe of cerulean examples to rerun ASA on. Define the names used for accessing S1 Scene ID and Cerulean Slick ID. In our case, the loaded dataset uses columns `scene_id` and `slick`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = cv3_path + r'\\slick_to_source dump 2024-12-31.csv'\n",
    "scene_slick_df = pd.read_csv(csv_path)\n",
    "scene_slick_df = scene_slick_df.iloc[[0]]#test case with only a single scene and slick\n",
    "\n",
    "scene_column = 'scene_id'\n",
    "slick_column = 'slick'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer, asa_type = AISAnalyzer, 'asa_ais'\n",
    "# analyzer, asa_type = InfrastructureAnalyzer, 'asa_infra'\n",
    "# analyzer, asa_type = DarkAnalyzer, 'asa_dark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define columns from analyzer outputs that will be saved in the rerun dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'scene_id', 'slick_id', 'st_name', 'coincidence_score', 'overlap_score',\n",
    "    'temporal_score', 'distance_score', 'collated_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ASA on the Cerulean examples loaded above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:11<00:00, 191.23s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "rerun_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for (scene_id, scene_group) in tqdm(scene_slick_df.groupby(scene_column)):\n",
    "    print(scene_id)\n",
    "    s1_scene = get_s1_scene(scene_id)\n",
    "    asa = analyzer(s1_scene)\n",
    "    print(\"processing \",len(np.unique(scene_group[slick_column].values)), \"true slicks...\")\n",
    "\n",
    "    for slick_id,slick_group in scene_group.groupby(slick_column):\n",
    "        geojson_file_path = download_geojson(slick_id)\n",
    "        slick_gdf = gpd.read_file(geojson_file_path)\n",
    "        res = asa.compute_coincidence_scores(slick_gdf)\n",
    "        \n",
    "        for i,sel_res in res.iterrows():\n",
    "            # Start with the first two keys assigned from variables.\n",
    "            insert_row = {\n",
    "                'scene_id': scene_id,\n",
    "                'slick_id': slick_id\n",
    "            }\n",
    "            # Now add the rest of the keys from sel_res.\n",
    "            for col in columns:\n",
    "                if col in ['scene_id', 'slick_id']:\n",
    "                    continue  # Already set above.\n",
    "                insert_row[col] = sel_res.get(col)\n",
    "\n",
    "            new_row = pd.DataFrame([insert_row])\n",
    "            rerun_df = pd.concat([rerun_df, new_row], ignore_index=True)\n",
    "  \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save resulting ASA outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-13\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = cv3_path\n",
    "save_file_name = f\"{save_folder}\\{asa_type}_local_run_{current_date}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_df.to_csv(save_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
