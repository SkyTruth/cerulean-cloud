{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzer Evaluation Demonstration Notebook\n",
    "\n",
    "This notebook demonstrates how to use the functions in `source_analyzer_evaluation.py` to evaluate three analyzers:\n",
    "\n",
    "- **AISAnalyzer**: Run on a groundtruth dataset called `vessel_groundtruth`.\n",
    "- **InfrastructureAnalyzer**: Run on a dataset called `infrastructure_groundtruth`.\n",
    "- **DarkAnalyzer**: Run on a dataset called `dark_vessel_groundtruth`.\n",
    "\n",
    "For AISAnalyzer and InfrastructureAnalyzer, we label results using the source name (`st_name`). For DarkAnalyzer, we label results based on the spatial distance (using a 0.005° threshold) to the ground truth dark vessel location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set the download path for demonstration and ensure the folder exists.\n",
    "download_path = os.getenv(\"ASA_DOWNLOAD_PATH\")\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "git_path = os.getenv(\"GIT_FOLDER\")\n",
    "cv3_path = os.getenv(\"CV3_FOLDER\")\n",
    "sys.path.append(git_path)\n",
    "sys.path.append(cv3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerulean_cloud.cloud_function_asa.utils.analyzer import (\n",
    "    AISAnalyzer,\n",
    "    InfrastructureAnalyzer,\n",
    "    DarkAnalyzer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from your script.\n",
    "from source_analyzer_evaluation import (\n",
    "    label_dark_vessel_results_with_distance,\n",
    "    label_results_with_st_name,\n",
    "    apply_labeling,\n",
    "    calculate_metrics,\n",
    "    plot_metrics,\n",
    "    process_groundtruth_on_analyzer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Groundtruth Datasets\n",
    "\n",
    "Load the dark vessel dataset, dark vessel SAR detections, hitl verification for vessels infrastructure, and gfw infrastructure data. Be sure to change `eval_folder` to point to where these datasets are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_folder = os.path.join(cv3_path, \"asa_analysis/evaluation\")\n",
    "refined_dark_vessel_dataset = os.path.join(\n",
    "    eval_folder, \"refined_dark_vessel_dataset.csv\"\n",
    ")\n",
    "sar_detections_hitl_dark_ds = os.path.join(\n",
    "    eval_folder, \"sar_detections_hitl_dark_ds.csv\"\n",
    ")\n",
    "slick_to_source_dump_2024_12_31 = os.path.join(\n",
    "    eval_folder, \"slick_to_source dump 2024-12-31.csv\"\n",
    ")\n",
    "nonoise_SAR_Fixed_Infrastructure = os.path.join(\n",
    "    eval_folder, \"nonoise_SAR_Fixed_Infrastructure.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data may require some post-processing to prepare it for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_vess_df = pd.read_csv(refined_dark_vessel_dataset)\n",
    "sar_detections = pd.read_csv(sar_detections_hitl_dark_ds)\n",
    "\n",
    "dark_vessel_groundtruth = gpd.GeoDataFrame(\n",
    "    dark_vess_df,\n",
    "    geometry=gpd.points_from_xy(dark_vess_df[\"lon\"], dark_vess_df[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    sar_detections,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        sar_detections[\"detect_lon\"], sar_detections[\"detect_lat\"]\n",
    "    ),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "sar_detections_gdf = sar_detections_gdf[sar_detections_gdf[\"structure_id\"].isna()]\n",
    "sar_detections_gdf = sar_detections_gdf.reset_index()\n",
    "\n",
    "hitl_df = pd.read_csv(slick_to_source_dump_2024_12_31)\n",
    "\n",
    "infrastructure_groundtruth = hitl_df[\n",
    "    (hitl_df[\"type\"] == 2) & (hitl_df[\"hitl_verification\"])\n",
    "]\n",
    "vessel_groundtruth = hitl_df[(hitl_df[\"type\"] == 1) & (hitl_df[\"hitl_verification\"])]\n",
    "\n",
    "if \"slick\" in vessel_groundtruth.columns:\n",
    "    vessel_groundtruth = vessel_groundtruth.rename(columns={\"slick\": \"slick_id\"})\n",
    "if \"slick\" in infrastructure_groundtruth.columns:\n",
    "    infrastructure_groundtruth = infrastructure_groundtruth.rename(\n",
    "        columns={\"slick\": \"slick_id\"}\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(nonoise_SAR_Fixed_Infrastructure)\n",
    "gfw_gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=[Point(xy) for xy in zip(df[\"lon\"], df[\"lat\"])], crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit number of slicks processed for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_vessel_groundtruth = vessel_groundtruth.iloc[range(1, 11)]\n",
    "SAMPLE_infrastructure_groundtruth = infrastructure_groundtruth.iloc[range(1, 11)]\n",
    "SAMPLE_dark_vessel_groundtruth = dark_vessel_groundtruth.iloc[range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Groundtruth with Each Analyzer\n",
    "\n",
    "We use the `process_groundtruth_on_analyzer` function to run each analyzer over its respective groundtruth.\n",
    "This function loops over each groundtruth row, downloads the associated slick and scene GeoJSON, and then computes\n",
    "coincidence scores using the given analyzer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AISAnalyzer on vessel_groundtruth.\n",
    "results_vessel = process_groundtruth_on_analyzer(\n",
    "    AISAnalyzer, SAMPLE_vessel_groundtruth, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process InfrastructureAnalyzer on infrastructure_groundtruth.\n",
    "results_infra = process_groundtruth_on_analyzer(\n",
    "    InfrastructureAnalyzer,\n",
    "    SAMPLE_infrastructure_groundtruth,\n",
    "    points_gdf=gfw_gdf,\n",
    "    analyzer_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process DarkAnalyzer on dark_vessel_groundtruth.\n",
    "results_dark = process_groundtruth_on_analyzer(\n",
    "    DarkAnalyzer,\n",
    "    SAMPLE_dark_vessel_groundtruth,\n",
    "    points_gdf=sar_detections_gdf,\n",
    "    analyzer_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the Results\n",
    "\n",
    "For the AIS and Infrastructure analyzers, we label detections using the `st_name` by applying `label_results_with_st_name`. Both the results and the groundtruth dataframes must have `st_name` as the source identifier with type `int`.\n",
    "\n",
    "For the DarkAnalyzer, we label based on distance using `label_dark_vessel_results_with_distance`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess outputs if necessary to align with groundtruth\n",
    "results_infra[\"st_name\"] = results_infra[\"structure_id\"]\n",
    "results_vessel[\"st_name\"] = results_vessel[\"st_name\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vessel_labeled = apply_labeling(\n",
    "    results_vessel, SAMPLE_vessel_groundtruth, label_results_with_st_name\n",
    ")\n",
    "results_infra_labeled = apply_labeling(\n",
    "    results_infra, SAMPLE_infrastructure_groundtruth, label_results_with_st_name\n",
    ")\n",
    "# Label dark vessel results based on distance.\n",
    "results_dark_labeled = apply_labeling(\n",
    "    results_dark,\n",
    "    SAMPLE_dark_vessel_groundtruth,\n",
    "    label_dark_vessel_results_with_distance,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Display Metrics\n",
    "\n",
    "We now compute evaluation metrics (e.g., top-1 and top-3 source rates, average coincidence scores, and the ratio of true to false coincidence scores)\n",
    "for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"AISAnalyzer\": results_vessel_labeled,\n",
    "    \"InfrastructureAnalyzer\": results_infra_labeled,\n",
    "    \"DarkAnalyzer\": results_dark_labeled,\n",
    "}\n",
    "\n",
    "metrics_df = calculate_metrics(all_results)\n",
    "print(\"Evaluation Metrics:\")\n",
    "plot_metrics(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Timing output for processing? benchmark how long algos take\n",
    "\n",
    "• collation mean/stdevs for non-vessel (if vessel is run too)\n",
    "\n",
    "• messy issue: duplicate data: an object appears in both infra and AIS datasets which means when calculating collated scores, some of the infra solutions will have to be handled in a bespoke way (to ignore, or essentially accept as correct any infra that is also broadcasting to AIS, as either of those is avtually a valid answer)? Can we ensure that the true infra is above the true vessel? (end result might be that we end up pushing down on average the AIS results, in favor of the infra results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceru202411",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
