{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzer Evaluation Demonstration Notebook\n",
    "\n",
    "This notebook demonstrates how to use the functions in `source_analyzer_evaluation.py` to evaluate three analyzers:\n",
    "\n",
    "- **AISAnalyzer**: Run on a groundtruth dataset called `vessel_groundtruth`.\n",
    "- **InfrastructureAnalyzer**: Run on a dataset called `infrastructure_groundtruth`.\n",
    "- **DarkAnalyzer**: Run on a dataset called `dark_vessel_groundtruth`.\n",
    "\n",
    "For AISAnalyzer and InfrastructureAnalyzer, we label results using the source name (`st_name`). For DarkAnalyzer, we label results based on the spatial distance (using a 0.005Â° threshold) to the ground truth dark vessel location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from IPython.display import clear_output\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set the download path for demonstration and ensure the folder exists.\n",
    "download_path = os.getenv(\"ASA_DOWNLOAD_PATH\")\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "git_path = os.getenv(\"GIT_FOLDER\")\n",
    "cv3_path = os.getenv(\"CV3_FOLDER\") \n",
    "sys.path.append(git_path)\n",
    "sys.path.append(cv3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerulean_cloud.cloud_function_asa.utils.analyzer import (\n",
    "        AISAnalyzer,\n",
    "        InfrastructureAnalyzer,\n",
    "        DarkAnalyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from your script.\n",
    "from asa_analysis.evaluation.source_analyzer_evaluation import (\n",
    "    label_dark_vessel_results_with_distance,\n",
    "    label_results_with_st_name,\n",
    "    apply_labeling,\n",
    "    calculate_metrics,\n",
    "    plot_metrics,\n",
    "    process_groundtruth_on_analyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Groundtruth Datasets\n",
    "\n",
    "Load the dark vessel dataset and the SAR detections. Then, load the CSV that contains hitl verification for vessels and infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_vess_df = pd.read_csv(r'refined_dark_vessel_dataset.csv')\n",
    "sar_detections = pd.read_csv(r'sar_detections_hitl_dark_ds.csv')\n",
    "\n",
    "# Keep only rows where 'mmsi' is NaN (i.e. true dark vessel detections)\n",
    "# true_dark_vess_df = dark_vess_df[dark_vess_df['mmsi'].isna()]\n",
    "\n",
    "# Toggle long distance coincidence: remove vessels ~20km or more away from slick.\n",
    "long_distance_coincidence = [3581643, 3581482, 3581103, 3581287, 3581532, 3581538,\n",
    "                             3582446, 3581900, 3582053, 3580996, 3581711, 3581920,\n",
    "                             3581075, 3581141, 3581812, 3582235, 3582465, 3582774, 3582584]\n",
    "dark_vess_df = dark_vess_df[[(slick_id not in long_distance_coincidence)\n",
    "                              for slick_id in dark_vess_df['slick_id'].values]]\n",
    "\n",
    "# Create a GeoDataFrame for dark vessels.\n",
    "dark_vessel_groundtruth = gpd.GeoDataFrame(\n",
    "    dark_vess_df, \n",
    "    geometry=gpd.points_from_xy(dark_vess_df['lon'], dark_vess_df['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Process SAR detections (for example purposes; not used further in this demo).\n",
    "sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    sar_detections, \n",
    "    geometry=gpd.points_from_xy(sar_detections['detect_lon'], sar_detections['detect_lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "sar_detections_gdf = sar_detections_gdf[sar_detections_gdf['structure_id'].isna()]\n",
    "sar_detections_gdf = sar_detections_gdf.reset_index()\n",
    "\n",
    "# Load hitl CSV for vessel and infrastructure groundtruth.\n",
    "csv_path = r'slick_to_source dump 2024-12-31.csv'\n",
    "hitl_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract groundtruth lists for infrastructure and vessels.\n",
    "infrastructure_groundtruth = hitl_df[(hitl_df['type'] == 2) & (hitl_df['hitl_verification'])]\n",
    "vessel_groundtruth = hitl_df[(hitl_df['type'] == 1) & (hitl_df['hitl_verification'])]\n",
    "\n",
    "if \"slick\" in vessel_groundtruth.columns:\n",
    "    vessel_groundtruth = vessel_groundtruth.rename(columns={\"slick\": \"slick_id\"})\n",
    "if \"slick\" in infrastructure_groundtruth.columns:\n",
    "    infrastructure_groundtruth = infrastructure_groundtruth.rename(columns={\"slick\": \"slick_id\"})\n",
    "\n",
    "df = pd.read_csv(r'nonoise_SAR_Fixed_Infrastructure.csv')\n",
    "gfw_gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=[Point(xy) for xy in zip(df['lon'], df['lat'])],\n",
    "    crs=\"EPSG:4326\"  # Set the coordinate reference system to WGS84\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_groundtruth = vessel_groundtruth.iloc[[1,2,3,4,5,6,7,8,9,10]]\n",
    "# infrastructure_groundtruth = infrastructure_groundtruth.iloc[[0]]\n",
    "# dark_vessel_groundtruth = dark_vessel_groundtruth.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Groundtruth with Each Analyzer\n",
    "\n",
    "We use the `process_groundtruth_on_analyzer` function to run each analyzer over its respective groundtruth.\n",
    "This function loops over each groundtruth row, downloads the associated slick and scene GeoJSON, and then computes\n",
    "coincidence scores using the given analyzer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process InfrastructureAnalyzer on infrastructure_groundtruth.\n",
    "results_infra = process_groundtruth_on_analyzer(\n",
    "    InfrastructureAnalyzer, infrastructure_groundtruth, points_gdf=gfw_gdf, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process DarkAnalyzer on dark_vessel_groundtruth.\n",
    "results_dark = process_groundtruth_on_analyzer(\n",
    "    DarkAnalyzer, dark_vessel_groundtruth, points_gdf=sar_detections_gdf, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vessel = process_groundtruth_on_analyzer(\n",
    "    AISAnalyzer, vessel_groundtruth, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the Results\n",
    "\n",
    "For the AIS and Infrastructure analyzers, we label detections using the `st_name` by applying `label_results_with_st_name`. Both the results and the groundtruth dataframes must have `st_name` as the source identifier with type `int`.\n",
    "\n",
    "For the DarkAnalyzer, we label based on distance using `label_dark_vessel_results_with_distance`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocess outputs if necessary to align with groundtruth\n",
    "results_infra['st_name']=results_infra['structure_id']\n",
    "results_vessel['st_name'] = results_vessel['st_name'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vessel_labeled = apply_labeling(results_vessel, vessel_groundtruth, label_results_with_st_name)\n",
    "results_infra_labeled = apply_labeling(results_infra, infrastructure_groundtruth, label_results_with_st_name)\n",
    "# Label dark vessel results based on distance.\n",
    "results_dark_labeled = apply_labeling(results_dark, dark_vessel_groundtruth, label_dark_vessel_results_with_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Display Metrics\n",
    "\n",
    "We now compute evaluation metrics (e.g., top-1 and top-3 source rates, average coincidence scores, and the ratio of true to false coincidence scores)\n",
    "for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"AISAnalyzer\": results_vessel_labeled,\n",
    "    \"InfrastructureAnalyzer\": results_infra_labeled,\n",
    "    \"DarkAnalyzer\": results_dark_labeled\n",
    "}\n",
    "\n",
    "metrics_df = calculate_metrics(all_results)\n",
    "print(\"Evaluation Metrics:\")\n",
    "plot_metrics(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
