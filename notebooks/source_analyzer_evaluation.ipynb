{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzer Evaluation Demonstration Notebook\n",
    "\n",
    "This notebook demonstrates how to use the functions in `source_analyzer_evaluation.py` to evaluate three analyzers:\n",
    "\n",
    "- **AISAnalyzer**: Run on a groundtruth dataset called `vessel_groundtruth`.\n",
    "- **InfrastructureAnalyzer**: Run on a dataset called `infrastructure_groundtruth`.\n",
    "- **DarkAnalyzer**: Run on a dataset called `dark_vessel_groundtruth`.\n",
    "\n",
    "For AISAnalyzer and InfrastructureAnalyzer, we label results using the source name (`st_name`). For DarkAnalyzer, we label results based on the spatial distance (using a 0.005Â° threshold) to the ground truth dark vessel location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from IPython.display import clear_output\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set the download path for demonstration and ensure the folder exists.\n",
    "download_path = os.getenv(\"ASA_DOWNLOAD_PATH\")\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "git_path = os.getenv(\"GIT_FOLDER\")\n",
    "cv3_path = os.getenv(\"CV3_FOLDER\") \n",
    "sys.path.append(git_path)\n",
    "sys.path.append(cv3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerulean_cloud.cloud_function_asa.utils.analyzer import (\n",
    "        AISAnalyzer,\n",
    "        InfrastructureAnalyzer,\n",
    "        DarkAnalyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from your script.\n",
    "from source_analyzer_evaluation import (\n",
    "    label_dark_vessel_results_with_distance,\n",
    "    label_results_with_st_name,\n",
    "    apply_labeling,\n",
    "    calculate_metrics,\n",
    "    plot_metrics,\n",
    "    process_groundtruth_on_analyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Groundtruth Datasets\n",
    "\n",
    "Load the dark vessel dataset, dark vessel SAR detections, hitl verification for vessels infrastructure, and gfw infrastructure data. Be sure to change `eval_folder` to point to where these datasets are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_folder = os.path.join(cv3_path, \"asa_analysis/evaluation\")\n",
    "refined_dark_vessel_dataset = os.path.join(eval_folder, 'refined_dark_vessel_dataset.csv')\n",
    "sar_detections_hitl_dark_ds = os.path.join(eval_folder, 'sar_detections_hitl_dark_ds.csv')\n",
    "slick_to_source_dump_2024_12_31 = os.path.join(eval_folder, 'slick_to_source dump 2024-12-31.csv')\n",
    "nonoise_SAR_Fixed_Infrastructure = os.path.join(eval_folder, 'nonoise_SAR_Fixed_Infrastructure.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data may require some post-processing to prepare it for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_vess_df = pd.read_csv(refined_dark_vessel_dataset)\n",
    "sar_detections = pd.read_csv(sar_detections_hitl_dark_ds)\n",
    "\n",
    "dark_vessel_groundtruth = gpd.GeoDataFrame(\n",
    "    dark_vess_df, \n",
    "    geometry=gpd.points_from_xy(dark_vess_df['lon'], dark_vess_df['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "sar_detections_gdf = gpd.GeoDataFrame(\n",
    "    sar_detections, \n",
    "    geometry=gpd.points_from_xy(sar_detections['detect_lon'], sar_detections['detect_lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "sar_detections_gdf = sar_detections_gdf[sar_detections_gdf['structure_id'].isna()]\n",
    "sar_detections_gdf = sar_detections_gdf.reset_index()\n",
    "\n",
    "hitl_df = pd.read_csv(slick_to_source_dump_2024_12_31)\n",
    "\n",
    "infrastructure_groundtruth = hitl_df[(hitl_df['type'] == 2) & (hitl_df['hitl_verification'] == True)]\n",
    "vessel_groundtruth = hitl_df[(hitl_df['type'] == 1) & (hitl_df['hitl_verification'] == True)]\n",
    "\n",
    "if \"slick\" in vessel_groundtruth.columns:\n",
    "    vessel_groundtruth = vessel_groundtruth.rename(columns={\"slick\": \"slick_id\"})\n",
    "if \"slick\" in infrastructure_groundtruth.columns:\n",
    "    infrastructure_groundtruth = infrastructure_groundtruth.rename(columns={\"slick\": \"slick_id\"})\n",
    "\n",
    "df = pd.read_csv(nonoise_SAR_Fixed_Infrastructure)\n",
    "gfw_gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=[Point(xy) for xy in zip(df['lon'], df['lat'])],\n",
    "    crs=\"EPSG:4326\" \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit number of slicks processed for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_groundtruth = vessel_groundtruth.iloc[[1,2,3,4,5,6,7,8,9,10]]\n",
    "# infrastructure_groundtruth = infrastructure_groundtruth.iloc[[0]]\n",
    "# dark_vessel_groundtruth = dark_vessel_groundtruth.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Groundtruth with Each Analyzer\n",
    "\n",
    "We use the `process_groundtruth_on_analyzer` function to run each analyzer over its respective groundtruth.\n",
    "This function loops over each groundtruth row, downloads the associated slick and scene GeoJSON, and then computes\n",
    "coincidence scores using the given analyzer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process InfrastructureAnalyzer on infrastructure_groundtruth.\n",
    "results_infra = process_groundtruth_on_analyzer(\n",
    "    InfrastructureAnalyzer, infrastructure_groundtruth, points_gdf=gfw_gdf, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process DarkAnalyzer on dark_vessel_groundtruth.\n",
    "results_dark = process_groundtruth_on_analyzer(\n",
    "    DarkAnalyzer, dark_vessel_groundtruth, points_gdf=sar_detections_gdf, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vessel = process_groundtruth_on_analyzer(\n",
    "    AISAnalyzer, vessel_groundtruth, analyzer_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the Results\n",
    "\n",
    "For the AIS and Infrastructure analyzers, we label detections using the `st_name` by applying `label_results_with_st_name`. Both the results and the groundtruth dataframes must have `st_name` as the source identifier with type `int`.\n",
    "\n",
    "For the DarkAnalyzer, we label based on distance using `label_dark_vessel_results_with_distance`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocess outputs if necessary to align with groundtruth\n",
    "results_infra['st_name']=results_infra['structure_id']\n",
    "results_vessel['st_name'] = results_vessel['st_name'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vessel_labeled = apply_labeling(results_vessel, vessel_groundtruth, label_results_with_st_name)\n",
    "results_infra_labeled = apply_labeling(results_infra, infrastructure_groundtruth, label_results_with_st_name)\n",
    "# Label dark vessel results based on distance.\n",
    "results_dark_labeled = apply_labeling(results_dark, dark_vessel_groundtruth, label_dark_vessel_results_with_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Display Metrics\n",
    "\n",
    "We now compute evaluation metrics (e.g., top-1 and top-3 source rates, average coincidence scores, and the ratio of true to false coincidence scores)\n",
    "for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"AISAnalyzer\": results_vessel_labeled,\n",
    "    \"InfrastructureAnalyzer\": results_infra_labeled,\n",
    "    \"DarkAnalyzer\": results_dark_labeled\n",
    "}\n",
    "\n",
    "metrics_df = calculate_metrics(all_results)\n",
    "print(\"Evaluation Metrics:\")\n",
    "plot_metrics(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
