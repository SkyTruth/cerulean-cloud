{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqXYtLmzgrRW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import GeometryCollection,Point, MultiPolygon, Polygon\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46ZSjK76Xlki"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzFhUtMMmquO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Automatic Source Analysis utils\n",
    "\"\"\"\n",
    "class ASA:\n",
    "    def __init__(self, version: int = 1, decay: float = 1.0, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the ASA class.\n",
    "\n",
    "        Parameters:\n",
    "        - version (int): Version of the function to use in confidence score calculations.\n",
    "        \"\"\"\n",
    "        self.version = version\n",
    "        self.decay = decay\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def estimate_utm_crs(self, geometry):\n",
    "        \"\"\"\n",
    "        Estimates an appropriate UTM CRS based on the centroid of the geometry.\n",
    "        \"\"\"\n",
    "        return CRS.from_dict(\n",
    "            {\n",
    "                \"proj\": \"utm\",\n",
    "                \"zone\": int((geometry.centroid.x + 180) / 6) + 1,\n",
    "                \"south\": geometry.centroid.y < 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def apply_closing_buffer(self, geo_df, closing_buffer):\n",
    "        \"\"\"\n",
    "        Applies a closing buffer to geometries in the GeoDataFrame.\n",
    "        \"\"\"\n",
    "        geo_df[\"geometry\"] = (\n",
    "            geo_df[\"geometry\"].buffer(closing_buffer).buffer(-closing_buffer)\n",
    "        )\n",
    "        return geo_df\n",
    "\n",
    "    def extract_polygons(self, geometry):\n",
    "        \"\"\"\n",
    "        Extracts individual polygons from a geometry.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            [geom for geom in geometry.geoms if isinstance(geom, Polygon)]\n",
    "            if isinstance(geometry, (MultiPolygon, GeometryCollection))\n",
    "            else [geometry]\n",
    "        )\n",
    "\n",
    "    def select_extreme_points(\n",
    "        self, polygon: Polygon, N: int, reference_points: List[np.ndarray]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects N extremity points from the polygon based on their distance from reference points.\n",
    "        \"\"\"\n",
    "        exterior_coords = np.array(polygon.exterior.coords[:-1])  # Exclude closing point\n",
    "        selected_points = []\n",
    "\n",
    "        for _ in range(N):\n",
    "            diff = exterior_coords[:, np.newaxis, :] - reference_points  # Shape: (M, K, 2)\n",
    "            dists = np.linalg.norm(diff, axis=2)  # Shape: (M, K)\n",
    "            min_dists = dists.min(axis=1)  # Shape: (M,)\n",
    "\n",
    "            idx = np.argmax(min_dists)\n",
    "            selected_point = exterior_coords[idx]\n",
    "            selected_points.append(selected_point)\n",
    "            reference_points.append(selected_point)\n",
    "\n",
    "        return np.array(selected_points)\n",
    "\n",
    "    def collect_extremity_points(\n",
    "        self,\n",
    "        polygons: List[Polygon],\n",
    "        N: int,\n",
    "        overall_centroid: np.ndarray,\n",
    "        largest_polygon_area: float,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Collects extremity points and their scaled area fractions from all polygons.\n",
    "        \"\"\"\n",
    "        extremity_points_list = []\n",
    "        area_fractions_list = []\n",
    "\n",
    "        MIN_AREA_THRESHOLD = 0.1 * largest_polygon_area\n",
    "\n",
    "        for polygon in polygons:\n",
    "            if polygon.area < MIN_AREA_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            selected_points = self.select_extreme_points(polygon, N, [overall_centroid])\n",
    "            extremity_points_list.append(selected_points)\n",
    "\n",
    "            area_fraction = polygon.area / largest_polygon_area\n",
    "            scaled_area_fraction = np.sqrt(area_fraction)\n",
    "            area_fractions_list.extend([scaled_area_fraction] * N)\n",
    "\n",
    "        if not extremity_points_list:\n",
    "            raise ValueError(\"No extremity points collected from polygons.\")\n",
    "\n",
    "        all_extremity_points = np.vstack(extremity_points_list)\n",
    "        all_area_fractions = np.array(area_fractions_list)\n",
    "\n",
    "        return all_extremity_points, all_area_fractions\n",
    "\n",
    "    def compute_weights(self, all_extremity_points, overall_centroid, all_area_fractions):\n",
    "        \"\"\"\n",
    "        Computes normalized weights based on distances from the centroid and area fractions.\n",
    "        \"\"\"\n",
    "        distances_sq = np.sum((all_extremity_points - overall_centroid) ** 2, axis=1)\n",
    "        scaled_weights = distances_sq * all_area_fractions\n",
    "        max_weight = scaled_weights.max()\n",
    "\n",
    "        return scaled_weights / max_weight if max_weight != 0 else np.ones_like(scaled_weights)\n",
    "\n",
    "    def compute_confidence_scores(\n",
    "        self,\n",
    "        infra_gdf: gpd.GeoDataFrame,\n",
    "        extremity_tree: cKDTree,\n",
    "        all_extremity_points: np.ndarray,\n",
    "        all_weights: np.ndarray,\n",
    "        decay: float,\n",
    "        radius_of_interest: float,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes confidence scores for infrastructure points based on proximity to extremity points.\n",
    "        \"\"\"\n",
    "        infra_coords = np.array([(geom.x, geom.y) for geom in infra_gdf.geometry])\n",
    "        extremity_indices = extremity_tree.query_ball_point(infra_coords, r=radius_of_interest)\n",
    "        confidence_scores = np.zeros(len(infra_coords))\n",
    "\n",
    "        for i, neighbors in enumerate(extremity_indices):\n",
    "            if neighbors:\n",
    "                neighbor_points = all_extremity_points[neighbors]\n",
    "                neighbor_weights = all_weights[neighbors]\n",
    "                dists = np.linalg.norm(neighbor_points - infra_coords[i], axis=1)\n",
    "                if self.verbose:\n",
    "                  print(\"decay\", self.decay)\n",
    "                  print(\"dists\", dists)\n",
    "                  print(\"radius_of_interest\", radius_of_interest)\n",
    "                  print(\"neighbor_weights\", neighbor_weights)\n",
    "                if self.version == 1:\n",
    "                    C_i = neighbor_weights - self.decay * dists / radius_of_interest\n",
    "                elif self.version == 2:\n",
    "                    C_i = neighbor_weights * (1 - self.decay * dists / radius_of_interest)\n",
    "                elif self.version == 3:\n",
    "                    C_i = neighbor_weights * np.exp(-self.decay * dists / radius_of_interest)\n",
    "\n",
    "                np.clip(C_i.max(), 0, 1)\n",
    "\n",
    "\n",
    "                confidence_scores[i] = np.clip(C_i.max(), 0, 1)\n",
    "\n",
    "        return confidence_scores\n",
    "\n",
    "    def associate_infra_to_slick(\n",
    "        self,\n",
    "        infra_gdf: gpd.GeoDataFrame,\n",
    "        slick_gdf: gpd.GeoDataFrame,\n",
    "        decay: float = 0.0003,\n",
    "        N: int = 10,\n",
    "        closing_buffer: int = 500,\n",
    "        radius_of_interest: int = 3000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Main function to compute confidence scores.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        crs_meters = self.estimate_utm_crs(slick_gdf.unary_union)\n",
    "        slick_gdf = slick_gdf.to_crs(crs_meters)\n",
    "        infra_gdf = infra_gdf.to_crs(crs_meters)\n",
    "\n",
    "        confidence_scores = np.zeros(len(infra_gdf))\n",
    "        slick_gdf = self.apply_closing_buffer(slick_gdf, closing_buffer)\n",
    "\n",
    "        combined_geometry = slick_gdf.unary_union\n",
    "        polygons = self.extract_polygons(combined_geometry)\n",
    "        slick_buffered = combined_geometry.buffer(radius_of_interest)\n",
    "\n",
    "        infra_within_radius = infra_gdf[infra_gdf.geometry.within(slick_buffered)]\n",
    "\n",
    "        if infra_within_radius.empty:\n",
    "            print(\"No infrastructure points within the radius of interest. Returning zero confidence scores.\")\n",
    "            return confidence_scores\n",
    "\n",
    "        infra_indices = infra_within_radius.index\n",
    "\n",
    "        largest_polygon_area = max(polygon.area for polygon in polygons)\n",
    "        overall_centroid = np.array(combined_geometry.centroid.coords[0])\n",
    "\n",
    "        all_extremity_points, all_area_fractions = self.collect_extremity_points(\n",
    "            polygons, N, overall_centroid, largest_polygon_area\n",
    "        )\n",
    "        all_weights = self.compute_weights(all_extremity_points, overall_centroid, all_area_fractions)\n",
    "\n",
    "        extremity_tree = cKDTree(all_extremity_points)\n",
    "        confidence_filtered = self.compute_confidence_scores(\n",
    "            infra_within_radius,\n",
    "            extremity_tree,\n",
    "            all_extremity_points,\n",
    "            all_weights,\n",
    "            decay,\n",
    "            radius_of_interest,\n",
    "        )\n",
    "\n",
    "        confidence_scores[infra_indices] = confidence_filtered\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Processing completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "        return confidence_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNJULOg9y427"
   },
   "outputs": [],
   "source": [
    "def plot_metrics_by_decay(decay_rates, true_association_scores, false_association_scores, top_source_rate, top_3_source_rate, avg_max_score, version=\"v1\"):\n",
    "    # Calculate True - False Association Scores\n",
    "    true_minus_false_scores = [t - f for t, f in zip(true_association_scores, false_association_scores)]\n",
    "\n",
    "    # Set up the figure and subplots in a single row\n",
    "    plt.figure(figsize=(24, 6))\n",
    "\n",
    "    # Set the main title for the figure\n",
    "    plt.suptitle(f\"Metrics by Decay Rate for version {version}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Generate positions for the decay rates (categorical x-axis)\n",
    "    x = np.arange(len(decay_rates))\n",
    "\n",
    "    # Subplot for True - False Association Scores by decay rate\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(x, true_minus_false_scores, marker='o', color='cornflowerblue')\n",
    "    plt.xlabel(\"Decay Rate\")\n",
    "    plt.ylabel(\"True - False Association Scores\")\n",
    "    plt.title(\"Difference of Avg Source Score and Avg Non-Source Score\")\n",
    "    plt.xticks(x, decay_rates)\n",
    "    plt.ylim(0, 1)\n",
    "    for i, v in enumerate(true_minus_false_scores):\n",
    "        plt.text(i, v + 0.01 if v >= 0 else v - 0.01, f\"{v:.3f}\", ha='center', va='bottom' if v >= 0 else 'top')\n",
    "\n",
    "    # Subplot for Top Source Rate by decay rate\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(x, top_source_rate, marker='o', color='gold')\n",
    "    plt.xlabel(\"Decay Rate\")\n",
    "    plt.ylabel(\"Top Source Rate\")\n",
    "    plt.title(\"Top Source Rate by Decay Rate\")\n",
    "    plt.xticks(x, decay_rates)\n",
    "    plt.ylim(0, 1.1)\n",
    "    for i, v in enumerate(top_source_rate):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "    # Subplot for Top 3 Source Rate by decay rate\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(x, top_3_source_rate, marker='o', color='mediumseagreen')\n",
    "    plt.xlabel(\"Decay Rate\")\n",
    "    plt.ylabel(\"Top 3 Source Rate\")\n",
    "    plt.title(\"Top 3 Source Rate by Decay Rate\")\n",
    "    plt.xticks(x, decay_rates)\n",
    "    plt.ylim(0, 1.1)\n",
    "    for i, v in enumerate(top_3_source_rate):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "    # Subplot for Avg Max Score by decay rate\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.plot(x, avg_max_score, marker='o', color='purple')\n",
    "    plt.xlabel(\"Decay Rate\")\n",
    "    plt.ylabel(\"Avg Max Score\")\n",
    "    plt.title(\"Avg Max Score of Non-Source by Decay Rate\")\n",
    "    plt.xticks(x, decay_rates)\n",
    "    plt.ylim(0, .5)\n",
    "    for i, v in enumerate(avg_max_score):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "def show_plots(true_association_scores, false_association_scores, top_source_rate, top_3_source_rate):\n",
    "    # Labels for versions and baseline\n",
    "    labels = ['v1', 'v2', 'v3', 'baseline']\n",
    "\n",
    "    # Define bar width and positions for each metric\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(labels))\n",
    "\n",
    "    # Calculate True minus False Association Scores\n",
    "    true_scores = [np.mean(scores) for scores in true_association_scores]\n",
    "    false_scores = [np.mean(scores) for scores in false_association_scores]\n",
    "    true_minus_false_scores = [t - f for t, f in zip(true_scores, false_scores)]\n",
    "\n",
    "    # Set up the figure and subplots in a single row\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Subplot for True - False Association Scores\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(x, true_minus_false_scores, color='cornflowerblue', width=bar_width)\n",
    "    plt.xlabel(\"Versions\")\n",
    "    plt.ylabel(\"True - False Association Scores\")\n",
    "    plt.title(\"Avg Source Score minus Avg non-source Score\")\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylim(0, 1.1)  # Set range from 0 to 1\n",
    "    for i, v in enumerate(true_minus_false_scores):\n",
    "        plt.text(i, v + 0.01 if v >= 0 else v - 0.01, f\"{v:.3f}\", ha='center', va='bottom' if v >= 0 else 'top')\n",
    "\n",
    "    # Subplot for Top Source Rate\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(x, top_source_rate, color='gold', width=bar_width)\n",
    "    plt.xlabel(\"Versions\")\n",
    "    plt.ylabel(\"Top Source Rate\")\n",
    "    plt.title(\"Top Source Rate by Version\")\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylim(0, 1.1)\n",
    "    for i, v in enumerate(top_source_rate):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "    # Subplot for Top 3 Source Rate\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(x, top_3_source_rate, color='mediumseagreen', width=bar_width)\n",
    "    plt.xlabel(\"Versions\")\n",
    "    plt.ylabel(\"Top 3 Source Rate\")\n",
    "    plt.title(\"Top 3 Source Rate by Version\")\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylim(0, 1.1)\n",
    "    for i, v in enumerate(top_3_source_rate):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4r6jlBgRebF"
   },
   "outputs": [],
   "source": [
    "def point_to_polygon_probability(points_gdf, polygons_gdf, decay_constant=200, buffer=.025):\n",
    "\n",
    "    probabilities = np.zeros(len(points_gdf))\n",
    "    points_gdf['orig_index'] = points_gdf.index\n",
    "\n",
    "    minx, miny, maxx, maxy = polygons_gdf.iloc[0].geometry.bounds\n",
    "    filtered_points = points_gdf[(points_gdf.geometry.x >= minx-buffer) & (points_gdf.geometry.x <= maxx+buffer) & (points_gdf.geometry.y >= miny-buffer) & (points_gdf.geometry.y <= maxy+buffer)]\n",
    "\n",
    "    for i,point in enumerate(filtered_points.geometry):\n",
    "        min_distance = float('inf')\n",
    "        for polygon in polygons_gdf.geometry:\n",
    "            if isinstance(polygon, MultiPolygon):\n",
    "                distance = min(point.distance(part.exterior) for part in polygon.geoms)\n",
    "            elif isinstance(polygon, Polygon):\n",
    "                distance = point.distance(polygon.exterior)\n",
    "            else:\n",
    "                raise TypeError(\"Polygon geometries must be either Polygon or MultiPolygon types.\")\n",
    "            min_distance = min(min_distance, distance)\n",
    "\n",
    "        probability = np.exp(-decay_constant * min_distance)\n",
    "        index = filtered_points.iloc[i]['orig_index']\n",
    "        probabilities[index] = probability\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVy-UaJ2-pM5"
   },
   "outputs": [],
   "source": [
    "def calculate_associated_infra(groundtruth_slicks, gfw_gdf, asa_algo):\n",
    "  associated_infra = []\n",
    "  for i in range(len(groundtruth_slicks)):\n",
    "    slick_of_interest = groundtruth_slicks.iloc[[i]]\n",
    "    probabilities = asa_algo(gfw_gdf, slick_of_interest)\n",
    "    # print(len(probabilities))\n",
    "    potential_sources = gfw_gdf[probabilities>0]\n",
    "    potential_sources['association_score'] = probabilities[probabilities>0]\n",
    "    potential_sources = potential_sources.sort_values(by='association_score', ascending=False)\n",
    "    associated_infra.append(potential_sources)\n",
    "  return associated_infra\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_false_score(associated_infra, groundtruth_slicks):\n",
    "    max_non_source_score = []\n",
    "    for i,potential_sources in enumerate(associated_infra):\n",
    "        false_association_scores = [0]\n",
    "        #Grab slick of interest and ground truth structure ids\n",
    "        slick_of_interest = groundtruth_slicks.iloc[[i]]\n",
    "        ground_truth_sources = [int(structure_id) for structure_id in slick_of_interest['structure_ids'].values[0].strip(\"[]\").split(',')]\n",
    "\n",
    "        #Grab scores and gfw structure ids associated with slick\n",
    "        scores = potential_sources['association_score'].values\n",
    "        struct_ids = potential_sources['structure_id'].values\n",
    "\n",
    "        #Grab the top potential source and record if it is a true or false source\n",
    "        #Record scores for true and false infra\n",
    "        for j,struct_id in enumerate(struct_ids):\n",
    "            if struct_id not in ground_truth_sources:\n",
    "                false_association_scores.append(scores[j])\n",
    "        max_non_source_score.append(max(false_association_scores))\n",
    "    return max_non_source_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-GfjQhRCnnD"
   },
   "outputs": [],
   "source": [
    "def scores_and_rates(associated_infra, groundtruth_slicks):\n",
    "  association_scores = []\n",
    "  top_source_rate = []\n",
    "  top_3_source_rate = []\n",
    "  total_sources = []\n",
    "\n",
    "  false_association_scores = []\n",
    "\n",
    "  for i,potential_sources in enumerate(associated_infra):\n",
    "\n",
    "    #Grab slick of interest and ground truth structure ids\n",
    "    slick_of_interest = groundtruth_slicks.iloc[[i]]\n",
    "    ground_truth_sources = [int(structure_id) for structure_id in slick_of_interest['structure_ids'].values[0].strip(\"[]\").split(',')]\n",
    "\n",
    "    #Grab scores and gfw structure ids associated with slick\n",
    "    scores = potential_sources['association_score'].values\n",
    "    struct_ids = potential_sources['structure_id'].values\n",
    "\n",
    "    #Grab the top potential source and record if it is a true or false source\n",
    "    top_source = struct_ids[0] if len(struct_ids) > 0 else None\n",
    "    top_source_rate.append(top_source in ground_truth_sources)\n",
    "\n",
    "    #Accumulate ground truth source ids\n",
    "    for ground_truth_source in ground_truth_sources:\n",
    "      total_sources.append(ground_truth_source)\n",
    "\n",
    "    #Record scores for true and false infra\n",
    "    for j,struct_id in enumerate(struct_ids):\n",
    "      if struct_id in ground_truth_sources:\n",
    "        association_scores.append(scores[j])\n",
    "      else:\n",
    "        false_association_scores.append(scores[j])\n",
    "\n",
    "    #Record if true source is among top 3 potential sources\n",
    "    source_in_top_3 = False\n",
    "    for j,struct_id in enumerate(struct_ids[0:3]):\n",
    "      if struct_id in ground_truth_sources:\n",
    "        source_in_top_3 = True\n",
    "        break\n",
    "    top_3_source_rate.append(source_in_top_3)\n",
    "\n",
    "  return association_scores, false_association_scores, top_source_rate, top_3_source_rate, total_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FB2BObyibfLf"
   },
   "outputs": [],
   "source": [
    "def generate_metrics(asa_v1,asa_v2,asa_v3,base_algo, gfw_gdf, groundtruth_slicks, unassociated_slicks):\n",
    "  associated_infra_v1 = calculate_associated_infra(groundtruth_slicks, gfw_gdf, asa_v1.associate_infra_to_slick)\n",
    "  associated_infra_v2 = calculate_associated_infra(groundtruth_slicks, gfw_gdf, asa_v2.associate_infra_to_slick)\n",
    "  associated_infra_v3 = calculate_associated_infra(groundtruth_slicks, gfw_gdf, asa_v3.associate_infra_to_slick)\n",
    "  associated_infra_baseline = calculate_associated_infra(groundtruth_slicks, gfw_gdf, base_algo)\n",
    "\n",
    "  unassociated_infra_v1 = calculate_associated_infra(unassociated_slicks, gfw_gdf, asa_v1.associate_infra_to_slick)\n",
    "  unassociated_infra_v2 = calculate_associated_infra(unassociated_slicks, gfw_gdf, asa_v2.associate_infra_to_slick)\n",
    "  unassociated_infra_v3 = calculate_associated_infra(unassociated_slicks, gfw_gdf, asa_v3.associate_infra_to_slick)\n",
    "  unassociated_infra_baseline = calculate_associated_infra(unassociated_slicks, gfw_gdf, base_algo)\n",
    "\n",
    "  non_source_max_v1 = highest_false_score(unassociated_infra_v1, groundtruth_slicks)\n",
    "  non_source_max_v2 = highest_false_score(unassociated_infra_v2, groundtruth_slicks)\n",
    "  non_source_max_v3 = highest_false_score(unassociated_infra_v3, groundtruth_slicks)\n",
    "  non_source_max_baseline = highest_false_score(unassociated_infra_baseline, groundtruth_slicks)\n",
    "\n",
    "  true_association_scores_v1, false_association_scores_v1, top_source_rate_v1, top_3_source_rate_v1, total_sources_v1 = scores_and_rates(associated_infra_v1, groundtruth_slicks)\n",
    "  true_association_scores_v2, false_association_scores_v2, top_source_rate_v2, top_3_source_rate_v2, total_sources_v2 = scores_and_rates(associated_infra_v2, groundtruth_slicks)\n",
    "  true_association_scores_v3, false_association_scores_v3, top_source_rate_v3, top_3_source_rate_v3, total_sources_v3 = scores_and_rates(associated_infra_v3, groundtruth_slicks)\n",
    "  true_association_scores_baseline, false_association_scores_baseline, top_source_rate_baseline, top_3_source_rate_baseline, total_sources_baseline = scores_and_rates(associated_infra_baseline, groundtruth_slicks)\n",
    "  true_association_scores = [sum(true_association_scores_v1)/len(true_association_scores_v1),\n",
    "                          sum(true_association_scores_v2)/len(true_association_scores_v2),\n",
    "                          sum(true_association_scores_v3)/len(true_association_scores_v3),\n",
    "                          sum(true_association_scores_baseline)/len(true_association_scores_baseline)]\n",
    "  false_association_scores = [sum(false_association_scores_v1)/len(false_association_scores_v1),\n",
    "                            sum(false_association_scores_v2)/len(false_association_scores_v2),\n",
    "                            sum(false_association_scores_v3)/len(false_association_scores_v3),\n",
    "                            sum(false_association_scores_baseline)/len(false_association_scores_baseline)]\n",
    "\n",
    "  top_source_rate = [sum(top_source_rate_v1)/len(top_source_rate_v1),\n",
    "                    sum(top_source_rate_v2)/len(top_source_rate_v2),\n",
    "                    sum(top_source_rate_v3)/len(top_source_rate_v3),\n",
    "                    sum(top_source_rate_baseline)/len(top_source_rate_baseline)]\n",
    "\n",
    "  top_3_source_rate = [sum(top_3_source_rate_v1)/len(top_3_source_rate_v1),\n",
    "                      sum(top_3_source_rate_v2)/len(top_3_source_rate_v2),\n",
    "                      sum(top_3_source_rate_v3)/len(top_3_source_rate_v3),\n",
    "                      sum(top_3_source_rate_baseline)/len(top_3_source_rate_baseline)]\n",
    "  \n",
    "  avg_max_non_source_score = [sum(non_source_max_v1)/len(non_source_max_v1),\n",
    "                              sum(non_source_max_v2)/len(non_source_max_v2),\n",
    "                              sum(non_source_max_v3)/len(non_source_max_v1),\n",
    "                              sum(non_source_max_baseline)/len(non_source_max_baseline)]\n",
    "\n",
    "  return true_association_scores, false_association_scores, top_source_rate, top_3_source_rate, avg_max_non_source_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hQOTgSNgqGl"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ebeva\\SkyTruth\\cv3\\infrastructure_validation_points.csv')\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "slick_gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "slick_gdf.crs = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rNnImiECJcd"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ebeva\\SkyTruth\\cv3\\nonoise_SAR_Fixed_Infrastructure.csv')\n",
    "gfw_gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=[Point(xy) for xy in zip(df['lon'], df['lat'])],\n",
    "    crs=\"EPSG:4326\"  # Set the coordinate reference system to WGS84\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePkcJR_t-Sb_",
    "outputId": "f5453e3f-788d-42e3-9e63-17fd9adc76a3"
   },
   "outputs": [],
   "source": [
    "groundtruth_slicks = slick_gdf[slick_gdf['structure_ids']!='[]']\n",
    "unassociated_slicks = slick_gdf[slick_gdf['structure_ids']=='[]']\n",
    "print(len(groundtruth_slicks))\n",
    "print(len(unassociated_slicks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMadI5wG0Eqz"
   },
   "source": [
    "Algorithm Versions Referenced in Analysis\n",
    "\n",
    "**Version 1**\n",
    "```\n",
    "C_i = neighbor_weights - decay * dists / radius_of_interest\n",
    "```\n",
    "**Version 2**\n",
    "```\n",
    "C_i = neighbor_weights * (1 - decay * dists / radius_of_interest)\n",
    "```\n",
    "**Version 3**\n",
    "```\n",
    "C_i = neighbor_weights * np.exp(-decay * dists / radius_of_interest)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "gVgnFXKkG4E_",
    "outputId": "16e28fcf-dcb7-43d5-fc0d-e6941b73c970"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=.01)\n",
    "asa_v2 = ASA(version=2,decay=.01)\n",
    "asa_v3 = ASA(version=3,decay=.01)\n",
    "true_association_scores_001, false_association_scores_001, top_source_rate_001, top_3_source_rate_001, avg_max_score_001 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_001, false_association_scores_001, top_source_rate_001, top_3_source_rate_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "HQBIv4S3Uxg9",
    "outputId": "f4614c3b-c564-440f-b662-b2f6b69bb608"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=.05)\n",
    "asa_v2 = ASA(version=2,decay=.05)\n",
    "asa_v3 = ASA(version=3,decay=.05)\n",
    "true_association_scores_005, false_association_scores_005, top_source_rate_005, top_3_source_rate_005, avg_max_score_005 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_005, false_association_scores_005, top_source_rate_005, top_3_source_rate_005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "cZM7KB-Mz6u7",
    "outputId": "eb9505a0-eb6a-44ee-f49e-b3dfb7030565"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=.1)\n",
    "asa_v2 = ASA(version=2,decay=.1)\n",
    "asa_v3 = ASA(version=3,decay=.1)\n",
    "true_association_scores_01, false_association_scores_01, top_source_rate_01, top_3_source_rate_01, avg_max_score_01 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_01, false_association_scores_01, top_source_rate_01, top_3_source_rate_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "OGApYauAWr55",
    "outputId": "2942972a-6a35-4a4f-d11b-bef7886de901"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=.5)\n",
    "asa_v2 = ASA(version=2,decay=.5)\n",
    "asa_v3 = ASA(version=3,decay=.5)\n",
    "true_association_scores_05, false_association_scores_05, top_source_rate_05, top_3_source_rate_05, avg_max_score_05 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_05, false_association_scores_05, top_source_rate_05, top_3_source_rate_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "g8jAzkiSYE9M",
    "outputId": "9437e834-8e00-431d-ae7c-711e1b3eabc0"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=1.0)\n",
    "asa_v2 = ASA(version=2,decay=1.0)\n",
    "asa_v3 = ASA(version=3,decay=1.0)\n",
    "true_association_scores_1, false_association_scores_1, top_source_rate_1, top_3_source_rate_1, avg_max_score_1 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_1, false_association_scores_1, top_source_rate_1, top_3_source_rate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "bDarqvucYI5c",
    "outputId": "7172eb51-29f4-4b01-f374-12d51e6c13f3"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=2.0)\n",
    "asa_v2 = ASA(version=2,decay=2.0)\n",
    "asa_v3 = ASA(version=3,decay=2.0)\n",
    "true_association_scores_2, false_association_scores_2, top_source_rate_2, top_3_source_rate_2, avg_max_score_2 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_2, false_association_scores_2, top_source_rate_2, top_3_source_rate_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "PDyF0Ri4YQ0x",
    "outputId": "82e3de1f-6cf5-46fa-cdc7-c3191486edb5"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=4.0)\n",
    "asa_v2 = ASA(version=2,decay=4.0)\n",
    "asa_v3 = ASA(version=3,decay=4.0)\n",
    "true_association_scores_4, false_association_scores_4, top_source_rate_4, top_3_source_rate_4, avg_max_score_4 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_4, false_association_scores_4, top_source_rate_4, top_3_source_rate_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Yae9qW1WY0tX",
    "outputId": "ee6aa0d8-99c3-48f0-bdd3-ec8318f8a595"
   },
   "outputs": [],
   "source": [
    "asa_v1 = ASA(version=1,decay=8.0)\n",
    "asa_v2 = ASA(version=2,decay=8.0)\n",
    "asa_v3 = ASA(version=3,decay=8.0)\n",
    "true_association_scores_8, false_association_scores_8, top_source_rate_8, top_3_source_rate_8, avg_max_score_8 = generate_metrics(asa_v1,asa_v2,asa_v3,point_to_polygon_probability, gfw_gdf, groundtruth_slicks, unassociated_slicks)\n",
    "clear_output()\n",
    "\n",
    "# show_plots(true_association_scores_8, false_association_scores_8, top_source_rate_8, top_3_source_rate_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lX0zoGrih2gf"
   },
   "outputs": [],
   "source": [
    "for v in range(0,3):\n",
    "  decay_rates = [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "  true_association_scores_v1 = [\n",
    "      true_association_scores_001[v],\n",
    "      true_association_scores_005[v],\n",
    "      true_association_scores_01[v],\n",
    "      true_association_scores_05[v],\n",
    "      true_association_scores_1[v],\n",
    "      true_association_scores_2[v],\n",
    "      true_association_scores_4[v],\n",
    "      true_association_scores_8[v],\n",
    "  ]\n",
    "\n",
    "  false_association_scores_v1 = [\n",
    "      false_association_scores_001[v],\n",
    "      false_association_scores_005[v],\n",
    "      false_association_scores_01[v],\n",
    "      false_association_scores_05[v],\n",
    "      false_association_scores_1[v],\n",
    "      false_association_scores_2[v],\n",
    "      false_association_scores_4[v],\n",
    "      false_association_scores_8[v],\n",
    "  ]\n",
    "\n",
    "  top_source_rate_v1 = [\n",
    "      top_source_rate_001[v],\n",
    "      top_source_rate_005[v],\n",
    "      top_source_rate_01[v],\n",
    "      top_source_rate_05[v],\n",
    "      top_source_rate_1[v],\n",
    "      top_source_rate_2[v],\n",
    "      top_source_rate_4[v],\n",
    "      top_source_rate_8[v],\n",
    "  ]\n",
    "\n",
    "  top_3_source_rate_v1 = [\n",
    "      top_3_source_rate_001[v],\n",
    "      top_3_source_rate_005[v],\n",
    "      top_3_source_rate_01[v],\n",
    "      top_3_source_rate_05[v],\n",
    "      top_3_source_rate_1[v],\n",
    "      top_3_source_rate_2[v],\n",
    "      top_3_source_rate_4[v],\n",
    "      top_3_source_rate_8[v],\n",
    "  ]\n",
    "  avg_max_non_source_score = [\n",
    "    avg_max_score_001[v],\n",
    "    avg_max_score_005[v],\n",
    "    avg_max_score_01[v],\n",
    "    avg_max_score_05[v],\n",
    "    avg_max_score_1[v],\n",
    "    avg_max_score_2[v],\n",
    "    avg_max_score_4[v],\n",
    "    avg_max_score_8[v]\n",
    "  ]\n",
    "  plot_metrics_by_decay(decay_rates, true_association_scores_v1, false_association_scores_v1, top_source_rate_v1, top_3_source_rate_v1, avg_max_non_source_score, version=v+1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
